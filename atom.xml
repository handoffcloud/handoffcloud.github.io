<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>handoff</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://handoff.cloud/"/>
  <updated>2022-02-10T05:57:34.884Z</updated>
  <id>http://handoff.cloud/</id>
  
  <author>
    <name>handoff</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Custom data engineering service for your startup operations</title>
    <link href="http://handoff.cloud/Introducing-handoff-cloud-for-startups/"/>
    <id>http://handoff.cloud/Introducing-handoff-cloud-for-startups/</id>
    <published>2022-02-10T00:00:00.000Z</published>
    <updated>2022-02-10T05:57:34.884Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Know-your-product-focus-amp-outsource-the-Data-Ops"><a href="#Know-your-product-focus-amp-outsource-the-Data-Ops" class="headerlink" title="Know your product focus &amp; outsource the Data Ops"></a>Know your product focus &amp; outsource the Data Ops</h2><p>If you are in a PLG (product-led growth) company, your engineering team should focus on the PRODUCT, not data operations.</p><p>But data analytics is the key to PLG strategy. So, How can we get the analytics engine going without distracting your precious (and expensive!) product engineering resources?</p><p>That is when you should check out handoff.cloud! We are here to help you OUTSOURCE data engineering needs.</p><p>handoff.cloud is LAUNCHING a new Data Integration Service for Startups.</p><video style="width: 100%; padding-bottom: 20px;" controls poster="/images/handoff-1min-intro-thumbnail.jpg">  <source src="https://handoff.cloud/assets/video/handoff-1min-intro.mp4" type="video/mp4">Your browser does not support the video tag.</video><h2 id="üôà-No-DATA-NO-VISIBILITY-Are-you-driving-your-company-at-the-top-speed-in-complete-darkness-üò±"><a href="#üôà-No-DATA-NO-VISIBILITY-Are-you-driving-your-company-at-the-top-speed-in-complete-darkness-üò±" class="headerlink" title="üôà No DATA, NO VISIBILITY: Are you driving your company at the top speed in complete darkness? üò±"></a>üôà No DATA, NO VISIBILITY: Are you driving your company at the top speed in complete darkness? üò±</h2><p>Hubspot, Salesforce, Zendesk‚Ä¶ I‚Äôm pretty sure you use tons of cloud applications to automate your business operations. Data is siloed in each application, including your own product.</p><p>How do you synthesize the data from zillion sources to get the whole picture of your customers?</p><p>You need an analytics engine (data warehouse and data pipelines) to bring the data to where decisions take place.</p><p><a href="https://handoff.cloud/book.html">üóìÔ∏è Book a FREE &amp; No Pressure Consulting session with us.</a></p><h2 id="Thought-Data-Engineering-is-expensive"><a href="#Thought-Data-Engineering-is-expensive" class="headerlink" title="$$$ Thought Data Engineering is expensive?"></a>$$$ Thought Data Engineering is expensive?</h2><p>Do not procrastinate implementing an analytics engine. Nor should you waste 6 months looking for a seasoned data engineer.</p><p>üí∏ The average data engineer in the USA costs $93,000 as base salary. The senior data engineers at FAANG companies are getting $300,000+ and even $500,000+ salary. üí∏</p><p>Can your startup afford that?</p><p>With handoff.cloud, you can have the modern data stack today. <strong>Our starting contract is LESS THAN $10K a year. WOW!</strong></p><p><a href="https://handoff.cloud/book.html">üóìÔ∏è Book a FREE &amp; No Pressure Consulting session with us.</a></p><h2 id="Case-Studies-See-how-we-helped-FAST-companies"><a href="#Case-Studies-See-how-we-helped-FAST-companies" class="headerlink" title="Case Studies: See how we helped FAST companies"></a>Case Studies: See how we helped FAST companies</h2><p>handoff.cloud is an end-to-end service for your data Extraction, Loading, and Transformation needs.</p><ul><li><strong>The product analytics team</strong> is an expert in SQL, but the analysis does not begin without the source data in the data warehouse.</li><li><strong>The marketing team</strong> should be evaluated for the quality of the leads, not quantity. Product qualified lead generation is the way to go for the freemium model, but you need custom engineering to close the feedback loop from the product usage.</li><li><strong>The customer success</strong> team should have well-defined customer health metrics. And they should be talking to at-risk customers, not spending time manually importing &amp; exporting data to Excel and Zendesk.</li></ul><p>Those are just a few examples of how we have helped fast companies. <strong>Read what our customers are saying about us:</strong></p><p>‚ÄúWe have been extremely pleased with the work done by ANELEN to set up a platform to enable us to gain deeper levels of insight into many aspects of our business. ANELEN has both the expertise and just as importantly, the desire to help its clients get the most value they can out of their data.‚Äù - Tim Thatcher COO, Tiny Technologies Inc.</p><p>‚ÄúANELEN‚Äôs team is absolutely outstanding. We closed our last round with a prominent VC in great part due to the data visibility we provided to investors through ANELEN‚Äôs work.‚Äù - Konstantin, Zvereff CEO, BlueCart Inc.</p><p><a href="https://handoff.cloud/book.html">üóìÔ∏è Book a FREE &amp; No Pressure Consulting session with us.</a></p><h2 id="A-tailor-made-service-built-on-cutting-edge-technology"><a href="#A-tailor-made-service-built-on-cutting-edge-technology" class="headerlink" title="A tailor-made service built on cutting-edge technology"></a>A tailor-made service built on cutting-edge technology</h2><p>Data engineering is hard. Serving data pipelines for many businesses is even harder.</p><p>It took a whole new approach to do it right. And we did it!</p><p><img src="/images/handoff-service-built-on-platform.png" alt="tech stack"></p><p>We architected handoff technology from zero, utilizing a unique serverless approach to data engineering operation. With this innovative design, our customers get:</p><ol><li><strong>Customization:</strong> We design data extraction, loading, transformation, and publishing logic exactly as you need.</li><li><strong>Reliability:</strong> A complete resource isolation that ensures reliable data delivery.</li><li><strong>Security:</strong> The best security practice: The data and process won‚Äôt go outside your cloud.</li></ol><p>‚Ä¶without worrying about maintaining the infrastructure!</p><p><a href="https://handoff.cloud/book.html">üóìÔ∏è Book a FREE &amp; No Pressure Consulting session with us.</a></p><h2 id="Active-Monitoring-Service-We‚Äôll-keep-our-eyes-on-the-data-flow-for-you"><a href="#Active-Monitoring-Service-We‚Äôll-keep-our-eyes-on-the-data-flow-for-you" class="headerlink" title="Active Monitoring Service: We‚Äôll keep our eyes on the data flow for you"></a>Active Monitoring Service: We‚Äôll keep our eyes on the data flow for you</h2><p>Thought you could get away by hiring a one-time contractor?</p><p>When you turn the faucet, you expect the water to flow.</p><p>That is true as long as you pay the utility bill that helps the water infra to operate at its best efficiency.</p><p>The same goes for data. The data pipeline needs monitoring and maintenance to deliver data as expected.</p><ul><li>The source application may have a breaking change.</li><li>They may update the data schema at any time.</li><li>The source data may become temporarily available.</li><li>What if there is a data quality concern?</li></ul><p>Until now, your only option was to hire a full-time data engineer.</p><p>üëÄ handoff changes the game.</p><p><strong>We have built a monitoring technology that lets us be the data watchdog</strong> for many businesses at a fraction of cost:</p><p><img src="/images/handoff-ui.gif" alt="monitoring"></p><p><a href="https://handoff.cloud/book.html">üóìÔ∏è Book a FREE &amp; No Pressure Consulting session with us.</a></p><p>Now you can reliably keep the data flowing to where decisions take place without hiring a data engineer. ANELEN‚Äôs mission is to help innovative businesses make smarter decisions with data. handoff.cloud achieves the engineering part of the big mission. Please send me an email at <a href="mailto:daigo@anelen.co">daigo@anelen.co</a> (.co not .com) anytime to discuss how I can help!</p><p>Daigo Tanaka, Ph.D.<br>CEO &amp; Data Scientist @ Anelen, Co., LLC<br><a href="https://linkedin.com/in/daigotanaka" target="_blank" rel="noopener">Connect with me on LinkedIn</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Know-your-product-focus-amp-outsource-the-Data-Ops&quot;&gt;&lt;a href=&quot;#Know-your-product-focus-amp-outsource-the-Data-Ops&quot; class=&quot;headerlink&quot;
      
    
    </summary>
    
    <content src="http://handoff.cloud//images/handoff-service-built-on-platform.png" type="image" />
    
    
    
  </entry>
  
  <entry>
    <title>Save data warehouse cost with delta view and partitioned raw table</title>
    <link href="http://handoff.cloud/Save-data-warehouse-cost-with-delta-view-and-partitioned-raw-table/"/>
    <id>http://handoff.cloud/Save-data-warehouse-cost-with-delta-view-and-partitioned-raw-table/</id>
    <published>2021-09-14T10:28:29.000Z</published>
    <updated>2022-02-10T05:57:34.884Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/delta-view.png" alt="delta view"></p><p>The worst nightmare of analytics managers is accidentally blowing up the data warehouse (DWH) cost. This could happen by accidentally running a query that scans the entire raw table that is huge. Even when the table is not at the petabyte scale, the accumulation of repeated queries could add up to.</p><p>The above scenario is especially true when you are using BigQuery, AWS Athena, or Redshift Spectrum whose cost is tied to the total data scanned by queries. When you are billed by the computational capacity purchase as in Snowplow and Redshift, you may be shielded from such incidents, but your DWH performance gets severely impacted by costly queries.</p><ul><li>How can we avoid receiving unexpectedly expensive bills from the BigQuery/Athena/Redshift Spectrum?</li><li>How can we keep our queries efficient so we don‚Äôt need to increase the over cost from DWH and operational inefficiency?</li></ul><p>This article introduces basic ideas that help you avoid the blowup and lower the overall cost.</p><h2 id="Enforcing-partitions-to-the-raw-tables"><a href="#Enforcing-partitions-to-the-raw-tables" class="headerlink" title="Enforcing partitions to the raw tables"></a>Enforcing partitions to the raw tables</h2><p>The first thing you can do is to enforce partitions on the raw tables. The big tables are almost always tied to the day-to-day ingestion of the raw data. The DWH can internally divide the big data into chunks, typically based on one of the timestamp columns in the table. The smaller chunks in the table are called partitions.</p><p>When you scan the partition with a specific range of timestamps, DWH scans only the partitions that fall into the range. That way, your query finishes fast with smaller data scanned. In most DWHs, you can set up to make the timestamp range always required by query to avoid scanning the entire table by accident.</p><p>Note: If you are a BigQuery user, <a href="https://github.com/anelendata/target-bigquery" target="_blank" rel="noopener">ANELEN‚Äôs target-bigquery-partition</a> loader does this automatically. We can also help you set up partitions for other DWHs. Just let us know by dropping an email to <a href="mailto:hello@handoff.cloud">hello@handoff.cloud</a>.</p><h2 id="Set-up-materialized-tables"><a href="#Set-up-materialized-tables" class="headerlink" title="Set up materialized tables"></a>Set up materialized tables</h2><p>The raw tables are cleaned and aggregated before they are put to use. You may be doing so by defining SQL views. Such views may be hooked to your dashboard. So, every time someone opens the dashboard, the DWH may be executing a query through the views.</p><p>You should pay extra attention to this if you are using dbt which encourages the use of views by design. Unless the view is configured to be automatically ‚Äúmaterialized‚Äù, or to cache the query result for the repeated requests, the view is tapping on the raw data each time someone sends the query. This could also cause a higher cost and performance issue.</p><p>One explicit way of avoiding the costly scanning of raw tables is to periodically replicate the view result into a separate table. Let‚Äôs call it materialized table. If your dashboard just needs daily updates, you can run a job to refresh the materialized table. Some DWHs such as BigQuery have schedule features so you don‚Äôt have to do it manually. Otherwise, you can define a job in Airflow or dbt cloud.</p><h2 id="Use-delta-view-to-get-fresh-data-efficiently"><a href="#Use-delta-view-to-get-fresh-data-efficiently" class="headerlink" title="Use delta view to get fresh data efficiently"></a>Use delta view to get fresh data efficiently</h2><p>One problem with the above approach is data freshness. Say, your data extractor (StitchData, Fivetran, Airflow, etc) pulls data from the source every hour but materialization happens daily basis. By accessing the materialized table, you only get yesterday‚Äôs metrics and of course, you want to peek at today‚Äôs intra-day progress. The so-called delta view will help you achieve that.</p><p>A delta view combines the raw data and the materialized table to synthesize the most recent data efficiently. First, it pulls out the pre-aggregated data from the materialized table. Then it checks the latest timestamp of the pulled data. Using the timestamp, it pulls the ‚Äúdelta‚Äù by scanning the raw table with the timestamp. This way, the delta view utilizes the best of both the raw table and the materialized table.</p><p>Here is an example dbt macro of delta view. The script also de-duplicates the rows based on the primary keys and the replication timestamp:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">&#123;% macro events_delta(</span><br><span class="line">    source_table,</span><br><span class="line">    primary_keys,</span><br><span class="line">    event_tstamp_key,</span><br><span class="line">    etl_tstamp_key,</span><br><span class="line">    all_keys&#x3D;[&quot;*&quot;],</span><br><span class="line">    historical_table&#x3D;None,</span><br><span class="line">    current_date&#x3D;&#39;current_date()&#39;,</span><br><span class="line">    lookback&#x3D;30)</span><br><span class="line">%&#125;</span><br><span class="line">&#x2F;*</span><br><span class="line">This query block is created by events_delta macro.</span><br><span class="line"></span><br><span class="line">1. events_delta checks the (hopefuly partitioned-) destination &#123;&#123; destination &#125;&#125;</span><br><span class="line">  table within the time-window &#123;&#123; lookback &#125;&#125; (days) since the last ingestion</span><br><span class="line">  &#123;&#123; etl_tstamp_key &#125;&#125;.</span><br><span class="line">2. The new data is identified in source &#123;&#123; source_table &#125;&#125; table having</span><br><span class="line">  &#123;&#123; etl_tsamp_key &#125;&#125; &gt; the last destination ETL timestamp.</span><br><span class="line">3. If the destination has an entry with the primary key, it will honor the</span><br><span class="line">  existing one. So, to replace the old one, you need to drop the partition</span><br><span class="line">  (or just that record) before running this query.</span><br><span class="line"></span><br><span class="line">Assumption: event time is always older than the etl time</span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line"></span><br><span class="line">&#123;% set primary_keys &#x3D; [&quot;pk-1&quot;, &quot;pk-2&quot;, &quot;pk-3&quot;,...] %&#125;</span><br><span class="line">&#123;% set all_keys &#x3D; [&quot;pk-1&quot;, &quot;pk-2&quot;, &quot;pk-3&quot;,..., &quot;non-pk-1&quot;, &quot;non-pk-2&quot;, &quot;non-pk-3&quot;, ...] %&#125;</span><br><span class="line">WITH</span><br><span class="line">source AS (</span><br><span class="line">    -- I&#39;m inserting a CTE to demonstrate pre-casting a field before calling the macro</span><br><span class="line">    select cast(timestamp as date) as date,</span><br><span class="line">    * except(timestamp, date)  -- This is a BigQuery dialect</span><br><span class="line">    from &#123;&#123; source(&#39;my_schema&#39;, &#39;my_raw_data_table&#39;) &#125;&#125;</span><br><span class="line">),</span><br><span class="line">&#123;&#123; events_delta(</span><br><span class="line">    &#39;source&#39;,</span><br><span class="line">    primary_keys,</span><br><span class="line">    &#39;date&#39;,</span><br><span class="line">    &#39;my_etl_timestamp&#39;,</span><br><span class="line">    all_keys&#x3D;all_keys,</span><br><span class="line">    historical_table&#x3D;source(&#39;my_schema&#39;, &#39;my_historical_table&#39;),</span><br><span class="line">    lookback&#x3D;30)</span><br><span class="line">&#125;&#125;</span><br><span class="line">select * from events_delta</span><br><span class="line">union all</span><br><span class="line">select * from source(&#39;my_schema&#39;, &#39;my_historical_table&#39;)</span><br><span class="line">*&#x2F;</span><br><span class="line"></span><br><span class="line">_events_delta_new AS (</span><br><span class="line">  SELECT</span><br><span class="line">&#123;% for key in all_keys %&#125;</span><br><span class="line">    n.&#123;&#123; key &#125;&#125;&#123;% if not loop.last %&#125;,&#123;% endif %&#125;</span><br><span class="line">&#123;% endfor %&#125;</span><br><span class="line">  FROM &#123;&#123; source_table &#125;&#125; AS n</span><br><span class="line">&#123;% if historical_table %&#125;</span><br><span class="line">  LEFT JOIN (</span><br><span class="line">    SELECT</span><br><span class="line">&#123;% for key in primary_keys %&#125;</span><br><span class="line">      &#123;&#123; key &#125;&#125;&#123;% if not loop.last %&#125;,&#123;% endif %&#125;</span><br><span class="line">&#123;% endfor %&#125;</span><br><span class="line">    FROM &#123;&#123; historical_table &#125;&#125;</span><br><span class="line">    WHERE DATE_ADD(&#123;&#123; current_date &#125;&#125;, INTERVAL -&#123;&#123; lookback &#125;&#125; day) &lt; CAST(&#123;&#123; event_tstamp_key &#125;&#125; AS DATE)</span><br><span class="line">  ) AS p</span><br><span class="line">      ON 1&#x3D;1</span><br><span class="line">&#123;% for key in primary_keys %&#125;</span><br><span class="line">            AND n.&#123;&#123; key &#125;&#125; &#x3D; p.&#123;&#123; key &#125;&#125;</span><br><span class="line">&#123;% endfor %&#125;</span><br><span class="line">&#123;% endif %&#125;</span><br><span class="line">  WHERE DATE_ADD(&#123;&#123; current_date &#125;&#125;, INTERVAL -&#123;&#123; lookback &#125;&#125; day) &lt;&#x3D; CAST(n.&#123;&#123; etl_tstamp_key &#125;&#125; AS DATE)</span><br><span class="line">    AND DATETIME_ADD(&#123;&#123; current_date &#125;&#125;, INTERVAL -&#123;&#123; lookback &#125;&#125; day) &lt; CAST(n.&#123;&#123; event_tstamp_key &#125;&#125; AS DATETIME)</span><br><span class="line">&#123;% if historical_table %&#125;</span><br><span class="line">&#123;% for key in primary_keys %&#125;</span><br><span class="line">    AND p.&#123;&#123; key &#125;&#125; IS NULL</span><br><span class="line">&#123;% endfor %&#125;</span><br><span class="line">&#123;% endif %&#125;</span><br><span class="line">),</span><br><span class="line">events_delta AS (</span><br><span class="line">  SELECT o.*</span><br><span class="line">  FROM _events_delta_new AS o</span><br><span class="line">  INNER JOIN (</span><br><span class="line">    SELECT</span><br><span class="line">&#123;% for key in primary_keys %&#125;</span><br><span class="line">      &#123;&#123; key &#125;&#125;,</span><br><span class="line">&#123;% endfor %&#125;</span><br><span class="line">      MAX(&#123;&#123; etl_tstamp_key &#125;&#125;) AS &#123;&#123; etl_tstamp_key &#125;&#125;,</span><br><span class="line">    FROM _events_delta_new</span><br><span class="line">    GROUP BY</span><br><span class="line">    &#123;% for key in primary_keys %&#125;</span><br><span class="line">    &#123;&#123; key &#125;&#125;&#123;% if not loop.last %&#125;,&#123;% endif %&#125;</span><br><span class="line">    &#123;% endfor %&#125;</span><br><span class="line">  ) AS oo</span><br><span class="line">ON o.&#123;&#123; etl_tstamp_key &#125;&#125; &#x3D; oo.&#123;&#123; etl_tstamp_key &#125;&#125;</span><br><span class="line">&#123;% for key in primary_keys %&#125;</span><br><span class="line">  AND o.&#123;&#123; key &#125;&#125; &#x3D; oo.&#123;&#123; key &#125;&#125;</span><br><span class="line">&#123;% endfor %&#125;</span><br><span class="line">)</span><br><span class="line">&#123;% endmacro %&#125;</span><br><span class="line">&#x2F;* This is the end of the query block generated by events_delta macro *&#x2F;</span><br></pre></td></tr></table></figure><p>You can use this delta view definition to update the materialized table. Then the updated materialized table is referred to by the delta view next time someone wants the up-to-date data.</p><h2 id="Seamlessly-run-extraction-and-materialization"><a href="#Seamlessly-run-extraction-and-materialization" class="headerlink" title="Seamlessly run extraction and materialization"></a>Seamlessly run extraction and materialization</h2><p>Delta view works by coordinating data extraction and materialization. You may do so by:</p><ol><li>Schedule data extraction by StitchData, Fivetran, or create a task in Airflow.</li><li>Define delta views. We recommend managing it with dbt with a macro like in the example above.</li><li>Schedule the materialization by DWH‚Äôs scheduling feature if available or set up a job by yourself.</li></ol><p>These steps may sound complex and the efficient coordination between the schedules at Step 1 &amp; 3 could be tricky.</p><p>Note: handoff.cloud can run all 3 steps together whenever the data is replicated. This seamless delta view process resolves the coordination issues and generates the most up-to-date data with the lowest query cost. Plus, our fully customizable service can integrate data from the applications not supported by existing connectors in the market. We can also support more complex ELT logic and custom transformations. Learn more at <a href="https://handoff.cloud">handoff.cloud</a></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>As your data and team grow bigger, the query cost and efficiency become a headache to analytics managers. The use of a partitioned table and the materialized table save you DWH cost and increase the cluster efficiency. Delta views are an efficient way of pulling up-to-date metrics while keeping the query cost low.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/images/delta-view.png&quot; alt=&quot;delta view&quot;&gt;&lt;/p&gt;
&lt;p&gt;The worst nightmare of analytics managers is accidentally blowing up the data 
      
    
    </summary>
    
    <content src="http://handoff.cloud//images/delta-view.png" type="image" />
    
    
    
  </entry>
  
  <entry>
    <title>Conversation with Pro: Adam Roderick @ Datateer</title>
    <link href="http://handoff.cloud/progressive-analytics-adam-roderick/"/>
    <id>http://handoff.cloud/progressive-analytics-adam-roderick/</id>
    <published>2021-09-05T00:00:00.000Z</published>
    <updated>2022-02-10T05:57:34.884Z</updated>
    
    <content type="html"><![CDATA[<p>A conversation with Adam Roderick, CEO of <a href="https://www.datateer.com" target="_blank" rel="noopener">Datateer</a>. Find out how he identifies high-value questions for the data to answer in short term.</p><video width="100%" controls poster="/images/progressive-analytics-adam-roderick.png">  <source src="https://handoff.cloud/assets/video/adam-roderick-progressive-analytics.m4v" type="video/x-m4v">Your browser does not support the video tag.</video>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;A conversation with Adam Roderick, CEO of &lt;a href=&quot;https://www.datateer.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Datateer&lt;/a&gt;. Find out how he
      
    
    </summary>
    
    <content src="http://handoff.cloud//images/progressive-analytics-adam-roderick.png" type="image" />
    
    
    
  </entry>
  
  <entry>
    <title>Video series: Conversation with users</title>
    <link href="http://handoff.cloud/Video-series-Conversation-with-users/"/>
    <id>http://handoff.cloud/Video-series-Conversation-with-users/</id>
    <published>2021-08-31T00:00:00.000Z</published>
    <updated>2022-02-10T05:57:34.884Z</updated>
    
    <content type="html"><![CDATA[<p>Video fun! Here we will compile our video based on the conversations with<br>our customers. Bite-size take aways for ELT best pratices from handoff üññ</p><video width="100%" controls poster="/images/handoff-ceo-excel.png">  <source src="https://handoff.cloud/assets/video/handoff-ceo-excel-square.m4v" type="video/x-m4v">Your browser does not support the video tag.</video><video width="100%" controls poster="/images/handoff-pql-pipeline.png">  <source src="https://handoff.cloud/assets/video/handoff-pql-pipeline.m4v" type="video/x-m4v">Your browser does not support the video tag.</video><video width="100%" controls poster="/images/open-source-elt.png">  <source src="https://handoff.cloud/assets/video/open-source-elt-square.m4v" type="video/x-m4v">Your browser does not support the video tag.</video><video width="100%" controls poster="/images/blt.png">  <source src="https://handoff.cloud/assets/video/blt-square.m4v" type="video/x-m4v">Your browser does not support the video tag.</video><video width="100%" controls poster="/images/how-not-to-do-serverless.png">  <source src="https://handoff.cloud/assets/video/how-not-to-do-serverless-square.m4v" type="video/x-m4v">Your browser does not support the video tag.</video>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Video fun! Here we will compile our video based on the conversations with&lt;br&gt;our customers. Bite-size take aways for ELT best pratices fr
      
    
    </summary>
    
    <content src="http://handoff.cloud//images/conversation-w-users-thumbnail.png" type="image" />
    
    
    
  </entry>
  
  <entry>
    <title>handoff founder interview</title>
    <link href="http://handoff.cloud/handoff-founder-interview/"/>
    <id>http://handoff.cloud/handoff-founder-interview/</id>
    <published>2021-07-27T23:56:07.000Z</published>
    <updated>2022-02-10T05:57:34.884Z</updated>
    
    <content type="html"><![CDATA[<p>Discover the advantages of using handoff ELT for data analytics.<br>Daigo Tanaka, founder of handoff.cloud answers everything you need to know about handoff.</p><video width="100%" controls poster="/images/founder-interview-poster.jpg">  <source src="https://handoff.cloud/assets/video/handoff-founder-interview.mp4" type="video/mp4">Your browser does not support the video tag.</video>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Discover the advantages of using handoff ELT for data analytics.&lt;br&gt;Daigo Tanaka, founder of handoff.cloud answers everything you need to
      
    
    </summary>
    
    <content src="http://handoff.cloud//images/founder-interview-poster.jpg" type="image" />
    
    
    
  </entry>
  
  <entry>
    <title>handoff 1 minute introduction video!</title>
    <link href="http://handoff.cloud/handoff-1min-intro-md/"/>
    <id>http://handoff.cloud/handoff-1min-intro-md/</id>
    <published>2021-07-21T23:24:41.000Z</published>
    <updated>2022-02-10T05:57:34.884Z</updated>
    
    <content type="html"><![CDATA[<p>Here is our first ever introduction video of handoff platform and service.<br>Please check out!</p><video width="100%" controls poster="/images/handoff-1min-intro.png">  <source src="https://handoff.cloud/assets/video/handoff-1min-intro.mp4" type="video/mp4">Your browser does not support the video tag.</video>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Here is our first ever introduction video of handoff platform and service.&lt;br&gt;Please check out!&lt;/p&gt;
&lt;video width=&quot;100%&quot; controls poster=&quot;
      
    
    </summary>
    
    <content src="http://handoff.cloud//images/handoff-1min-intro.png" type="image" />
    
    
    
  </entry>
  
  <entry>
    <title>handoff on Meltano Demo Day</title>
    <link href="http://handoff.cloud/handoff-on-Meltano-Demo-Day/"/>
    <id>http://handoff.cloud/handoff-on-Meltano-Demo-Day/</id>
    <published>2021-05-21T23:31:55.000Z</published>
    <updated>2022-02-10T05:57:34.884Z</updated>
    
    <content type="html"><![CDATA[<p>handoff‚Äôs Daigo Tanaka had the pleasure of sharing a demo of our ETL/ELT platform on Meltano‚Äôs Demo Day. If you can spare 10min please check out the video recording:</p><iframe width="100%" height="500px" src="https://www.youtube.com/embed/oHRon2xzeiw?start=1317" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;handoff‚Äôs Daigo Tanaka had the pleasure of sharing a demo of our ETL/ELT platform on Meltano‚Äôs Demo Day. If you can spare 10min please ch
      
    
    </summary>
    
    <content src="http://handoff.cloud//images/meltano-demo-day.png" type="image" />
    
    
    
  </entry>
  
  <entry>
    <title>Introducing Handoff: Serverless Data Pipeline Orchestration Framework</title>
    <link href="http://handoff.cloud/handoff-Serverless-Data-Pipeline-Orchestration-Framework/"/>
    <id>http://handoff.cloud/handoff-Serverless-Data-Pipeline-Orchestration-Framework/</id>
    <published>2021-03-20T13:47:40.000Z</published>
    <updated>2022-02-10T05:57:34.884Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://articles.anelen.co/images/keep-flowing-handoff.gif" alt="data-flow"></p><p>This article presents the business context and a technical deep dive of our serverless approach to data pipeline deployment. Our approach is <a href="https://github.com/anelendata/handoff" target="_blank" rel="noopener">open-sourced</a> as handoff: framework for serverless data pipeline orchestration.</p><h2 id="Business-Context-Helping-businesses-bring-any-data-anywhere"><a href="#Business-Context-Helping-businesses-bring-any-data-anywhere" class="headerlink" title="Business Context: Helping businesses bring any data anywhere"></a>Business Context: Helping businesses bring any data anywhere</h2><p>More and more companies have started to leverage cloud applications and their data in order to make their operation lean and effective. For example, D2C (direct to consumer) companies produce niche household products and ship directly to their customers instead of relying on a traditional retail distribution channel.</p><p>In doing so, a D2C company may use a marketing automation platform like Marketo or Pardot, run ads on Facebook and Google, and manage the supply chain with Flexport: All of these applications produce customer experience data. It is a common practice to extract data from cloud services and load them into a data warehouse (DWH). By combining data sources and analyzing them together, the business continuously improves the quality of the customer experience.</p><p>We are a technology-leveraged service company offering custom ETL/ELT solutions. ETL stands for Extracting, Transforming, and Loading of data. ELT workflows are becoming popular. Data are extracted and loaded into the data warehouse before transformation is executed by a massively parallel modern DWH engine.</p><p>The demand for implementing data pipelines is growing at an unprecedented pace. Data replication services such as Fivetran and StitchData provide data connectors for popular cloud applications such as Salesforce and Zendesk. However, those companies cannot support the long-tail of cloud applications in the Software as a Service (SaaS) market. If a cloud application is not supported, a business needs a custom data engineering job, but not every company can afford to developing an internal data engineering team.</p><p><img src="https://articles.anelen.co/images/long-tail.png" alt="No cloud apps left behind"></p><p>That is why fully-managed ETL services like ours are becoming the new option: With a fraction of the cost of hiring a data engineer, businesses can install the tailor-made data pipeline service.</p><p>While we serve our customers with active monitoring and maintenance, we decided to open-source the core framework as a contribution to data engineering community. In the rest of the article, we will share the technical solution that made our service possible.</p><h2 id="Technical-challenges-and-solution"><a href="#Technical-challenges-and-solution" class="headerlink" title="Technical challenges and solution"></a>Technical challenges and solution</h2><p>If you are the director of data engineering for your company‚Äôs internal operations, it makes sense to set up an Apache Airflow or subscribe to a managed service by Astronomer or Cloud Composer by Google Cloud Platform. Instead, we are a lean team helping others to perform the data integration of cloud applications in the long tail.</p><p>Simultaneously serving the data pipelines for many different clients change the technical requirements a lot. The most important part is to provide separate CPU, memory, and storage capacities for each client. We wanted to achieve this with maximum flexibility. When we started we only had a few data pipelines to manage, but we expected the number to rapidly grow.</p><h2 id="AWS-Lambda-vs-Fargate"><a href="#AWS-Lambda-vs-Fargate" class="headerlink" title="AWS Lambda vs. Fargate"></a>AWS Lambda vs. Fargate</h2><p>We went for a serverless approach. Our first prototype used Amazon Web Services (AWS) Lambda. It was quick to implement. The pay-as-you-go pricing was very attractive. We didn‚Äôt have to pay for idling the virtual machine or deal with peak-time auto-scaling challenges. But we soon realized that Lambda‚Äôs 15 minutes processing time limit is not enough for some tasks. So we switched to Fargate, another pay-as-you-go serverless platform by AWS. It has no time limit and the life-cycle management and scaling of computing resource is fully managed by the platform.</p><p>Compared to Lambda, Fargate‚Äôs deployment process is much more complex. Lambda could be deployed just by writing a script file. Fargate deployment involves Docker image creation, pushing the image to Elastic Container Registry, creating resources such as Virtual Private Cloud and Security Groups, defining the Fargate task, managing the Policy, and so on.</p><h2 id="Birth-of-handoff-A-serverless-data-pipeline-orchestration-framework"><a href="#Birth-of-handoff-A-serverless-data-pipeline-orchestration-framework" class="headerlink" title="Birth of handoff: A serverless data pipeline orchestration framework"></a>Birth of handoff: A serverless data pipeline orchestration framework</h2><p>The painful Fargate deployment process gave us the idea to implement a command-line interface (CLI) that we later named ‚Äúhandoff‚Äù. handoff simplifies the deployment steps for AWS Fargate. We may also provide a multi-cloud capability in the future: handoff unifies the commands to deploy to AWS, Microsoft Azure, Google Cloud Platform, and so on.</p><p>handoff describes the data pipeline logic in a YAML file. The first thing is to describe the installation of the modules required by the pipeline logic:</p><p>project.yml:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">version: 0.3</span><br><span class="line">description: Fetch foreign exchange rates</span><br><span class="line"></span><br><span class="line">installs:</span><br><span class="line">- venv: extractor</span><br><span class="line">  command: pip install tap-exchangeratesapi</span><br><span class="line">- venv: loader</span><br><span class="line">  command: pip install target-bigquery-partition</span><br></pre></td></tr></table></figure><p>As you can see, handoff is created primarily for Python. You can install modules in separate virtual environments if needed: handoff automatically generates multiple virtual environments. In local testing, you can install modules by running:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">handoff workspace install --project &lt;project_dir&gt; --workspace &lt;workspace_dir&gt;</span><br></pre></td></tr></table></figure><p>The pipeline logic is described as groups of tasks. In a task group, each command can be linked as a Unix pipeline:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tasks:</span><br><span class="line">- name: fetch_exchange_rates</span><br><span class="line">  description: Fetch exchange rates and load on BigQuery</span><br><span class="line">  pipeline:</span><br><span class="line">  - command: tap-exchangeratesapi</span><br><span class="line">    args: --config files&#x2F;tap-config.json</span><br><span class="line">    venv: extractor</span><br><span class="line">  - command: &quot;target-bigquery --config files&#x2F;target_config.json&quot;</span><br><span class="line">    venv: loader</span><br></pre></td></tr></table></figure><p>The pipeline is particularly useful in deploying memory-efficient tasks such as the ELT task by singer.io framework.</p><p>The following command runs the task locally:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">handoff run local -p &lt;project_dir&gt; -w &lt;workspace_dir&gt;</span><br></pre></td></tr></table></figure><h2 id="Doing-more-than-a-simple-data-replication-task"><a href="#Doing-more-than-a-simple-data-replication-task" class="headerlink" title="Doing more than a simple data replication task"></a>Doing more than a simple data replication task</h2><p>handoff also implements simple control flows such as for-each and fork. You can also define a task beyond simple ETL. Look how we automate the task of fetching stock market data, make a GIF animation, and post it on Twitter:</p><blockquote class="twitter-tweet"><p lang="en" dir="ltr">A fun demo: handoff&#39;s <a href="https://twitter.com/hashtag/datapipeline?src=hash&amp;ref_src=twsrc%5Etfw" target="_blank" rel="noopener">#datapipeline</a> can automatically harvest data and post data-driven content to Twitter <a href="https://twitter.com/hashtag/etl?src=hash&amp;ref_src=twsrc%5Etfw" target="_blank" rel="noopener">#etl</a> <a href="https://twitter.com/hashtag/analytics?src=hash&amp;ref_src=twsrc%5Etfw" target="_blank" rel="noopener">#analytics</a>. <a href="https://t.co/0ITR771mh9" target="_blank" rel="noopener">pic.twitter.com/0ITR771mh9</a></p>&mdash; handoff (@handoffcloud) <a href="https://twitter.com/handoffcloud/status/1377477316448636935?ref_src=twsrc%5Etfw" target="_blank" rel="noopener">April 1, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><h2 id="Deployment-features"><a href="#Deployment-features" class="headerlink" title="Deployment features"></a>Deployment features</h2><p>We estimate authoring pipeline logic is just 20% of the ETL development. To run the pipeline in production, we need to implement management of:</p><ul><li>Run-time configuration</li><li>Secret keys</li><li>Scheduling</li><li>Logging</li><li>Monitoring</li><li>Containerization<br>‚Ä¶and the list goes on. handoff‚Äôs project.yml helps to keep those organized and it is intuitive to understand. Here is an example of project.yml:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">vars:</span><br><span class="line">- key: gcp_project_id</span><br><span class="line">  value: &quot;handoff-demo&quot;</span><br><span class="line">- key: dataset_id</span><br><span class="line">  value: exchangerate</span><br><span class="line"></span><br><span class="line">envs:</span><br><span class="line">- key: GOOGLE_APPLICATION_CREDENTIALS</span><br><span class="line">  value: files&#x2F;google_client_secret.json</span><br><span class="line"></span><br><span class="line">deploy:</span><br><span class="line">  cloud_provider: aws</span><br><span class="line">  cloud_platform: fargate</span><br><span class="line">  resource_group: handoff-etl</span><br><span class="line">  container_image: tap-rest-api-target-bigquery</span><br><span class="line">  task: usgs-earthquakes</span><br><span class="line"></span><br><span class="line">schedules:</span><br><span class="line">- target_id: 1</span><br><span class="line">  description: Run everyday at 00:00:00Z</span><br><span class="line">  envs:</span><br><span class="line">  - key: __VARS</span><br><span class="line">    value: &#39;start_datetime&#x3D;$(date -Iseconds -d &quot;00:00 yesterday&quot;) end_datetime&#x3D;$(date -Iseconds -d &quot;00:00 today&quot;)&#39;</span><br><span class="line">  cron: &#39;0 0 * * ? *&#39;</span><br></pre></td></tr></table></figure><p>The commands to execute the containerization and the deployment are also simple and intuitive:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Push project configuration, supporting files, and secrets to AWS</span><br><span class="line">handoff project push -p &lt;project_dir&gt;</span><br><span class="line"></span><br><span class="line"># Build a Docker image and push to ECR</span><br><span class="line">handoff container build -p &lt;project_dir&gt;</span><br><span class="line">handoff container push -p &lt;project_dir&gt;</span><br><span class="line"></span><br><span class="line"># Create resource group, bucket, task...</span><br><span class="line">handoff cloud bucket create -p &lt;project_dir&gt;</span><br><span class="line">handoff cloud resources create -p &lt;project_dir&gt;</span><br><span class="line">handoff cloud task create -p &lt;project_dir&gt;</span><br><span class="line"></span><br><span class="line"># Schedule a task</span><br><span class="line">handoff cloud schedule create -p &lt;project_dir&gt;</span><br></pre></td></tr></table></figure><p>handoff simplifies development and deployment. Without handoff, you would have to author CloudFormation templates, deal with boto3 commands to transfer files to S3, figure out a way to encrypt and store the secrets, configure the CloudWatch for logging and metrics, etc.</p><p>For a full handoff tutorial, please visit <a href="https://dev.handoff.cloud" target="_blank" rel="noopener">https://dev.handoff.cloud</a></p><p>We made handoff <a href="https://github.com/anelendata/handoff" target="_blank" rel="noopener">a public project on GitHub</a> so every engineer having a similar challenge can use it. We also welcome collaborators to improve this new framework.</p><p><a href="https://articles.anelen.co/images/this_is_handoff.png" target="_blank" rel="noopener">handoff open source</a></p><h2 id="What‚Äôs-next"><a href="#What‚Äôs-next" class="headerlink" title="What‚Äôs next?"></a>What‚Äôs next?</h2><p>As we mentioned earlier, we may support deployment to other cloud platforms such as Microsoft Azure and Google Cloud Platform. Unifying the deployment commands for multi-cloud would be very useful in quickly switching platforms.</p><p>We also prototyped a web UI built on top of the CLI:</p><p><a href="https://articles.anelen.co/images/handoff-ui.gif" target="_blank" rel="noopener">dashboard</a></p><p>The web UI could be running as a stand-alone application on your local machine. It will spin up a local web server. You can interact with handoff via the web UI and the backend connects to cloud resources. It can also be hosted on a server for enterprise use cases.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>AWS Fargate lets us create a cost-effective and scalable solution for ETL deployment. However, it hasn‚Äôt been a popular approach due to the complex deployment process. handoff, a serverless data pipeline orchestration framework simplifies it. handoff also manages the various aspects of the orchestration such as security and monitoring. We made handoff an open-source project so that the data engineering community can benefit from it and collaborate for further innovation.</p><h2 id="About-the-author"><a href="#About-the-author" class="headerlink" title="About the author"></a>About the author</h2><p><a href="https://www.linkedin.com/in/daigotanaka/" target="_blank" rel="noopener">Daigo Tanaka</a> is CEO and data scientist at <a href="https://anelen.co/" target="_blank" rel="noopener">ANELEN</a>, a boutique consulting firm focused on data engineering, analytics, and data science. ANELEN‚Äôs mission is to help innovative businesses make smarter decisions with data science. ANELEN is the provider of handoff.could ETL/ELT service.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://articles.anelen.co/images/keep-flowing-handoff.gif&quot; alt=&quot;data-flow&quot;&gt;&lt;/p&gt;
&lt;p&gt;This article presents the business context 
      
    
    </summary>
    
    <content src="http://handoff.cloud/https://articles.anelen.co/images/keep-flowing-handoff.gif" type="image" />
    
    
    
  </entry>
  
  <entry>
    <title>What is P in ELTP Process?</title>
    <link href="http://handoff.cloud/What-is-P-in-ELTP-Process/"/>
    <id>http://handoff.cloud/What-is-P-in-ELTP-Process/</id>
    <published>2021-02-12T17:53:30.000Z</published>
    <updated>2022-02-10T05:57:34.884Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/iStock-1150767851.jpg" alt="data, where you need"></p><p><em>Solve the ‚Äúdata last-mile problem‚Äù to unlock the full analytics capabilities for your business.</em></p><h2 id="Adding-P-to-ELT"><a href="#Adding-P-to-ELT" class="headerlink" title="Adding P to ELT"></a>Adding P to ELT</h2><p>If you worked with a data-engineer, you may have heard the word ETL or ELT.</p><ul><li>E for Extracting data from the source.</li><li>T for Transforming from raw data to clean and useful information.</li><li>L for Loading to the destination to a data lake or data warehouse.</li></ul><p>The trend is to transform the data after loading the data to the powerful modern data warehouse such as Google BigQuery, Snowflake, AWS Redshift, and Microsoft Synapse. So, ELT rather than ETL is increasingly used to refer the data processing.</p><p>I‚Äôm adding P to ELT, and it‚Äôs ‚ÄúPublishing‚Äù. It‚Äôs the process of publishing the post-transformation data to exactly where people or another program consume them. We call a ‚Äúfrontline application‚Äù in this article.</p><h2 id="ELT-does-not-solve-the-‚ÄúData-Last-mile-Problem‚Äù"><a href="#ELT-does-not-solve-the-‚ÄúData-Last-mile-Problem‚Äù" class="headerlink" title="ELT does not solve the ‚ÄúData Last-mile Problem‚Äù"></a>ELT does not solve the ‚ÄúData Last-mile Problem‚Äù</h2><p>Let me expand this by a realistic business scenario: Product Qualified Lead scoring (PQL scoring).</p><p>Suppose your company is a Software-as-a-Service (SaaS) company. The service is free to sign up to use the basic feature. Your potential customer finds your service through an online ad. Sign up is free but the user must enter their work email and business information.</p><p>If you are using online marketing automation tools such as Marketo or Pardot, you can capture the prospective customer‚Äôs product discovery channel and their email at this point. But the marketing process typically lacks information on the usage statistics of your service. It‚Äôs because such data is typically sitting on the production database.</p><p>Or you may have taken one step further to replicate the data from the production database to the data warehouse through an ELT process. You may be a data scientist who came up with a formula or machine learning algorithm to compute a PQL score to indicate which free-tier users would likely convert to paying customers.</p><p>But as long as the data is sitting in the warehouse, it‚Äôs not going to be utilized. In the case of the PQL score like above, such score should be published to Marketo, Pardot, or Salesforce because that is where the sales and marketing staff do their job. They are too busy to open a business intelligence tool or run queries to find which prospects should be prioritized.</p><p><img src="/images/eltp.png" alt="ELTP process"></p><h2 id="Publish-Push-the-data-out-of-the-warehouse"><a href="#Publish-Push-the-data-out-of-the-warehouse" class="headerlink" title="Publish: Push the data out of the warehouse"></a>Publish: Push the data out of the warehouse</h2><p>The importance of publishing the metrics to the frontline applications is critical beyond the product marketing use cases. Another compelling case for SaaS business is customer success. For a subscription-based service, it is crucial to track the health of each subscriber account. Especially for complex business applications, the customer may give up on the product before seeing the value. Are your customers taking the right steps towards the value after signing up, or are they having difficulties starting out?</p><p>Enterprise SaaS companies typically have a customer success function to help the new account in the onboarding process and beyond. The product usage statistics and account‚Äôs health score would be very helpful only if such information is available right where they do their work such as Zendesk.</p><p>ELTP‚Äôs P takes care of the last mile problem of information delivery to make the business operation smart, efficient, and lean.</p><h2 id="ELTP-Automation"><a href="#ELTP-Automation" class="headerlink" title="ELTP Automation"></a>ELTP Automation</h2><p>Over the years, the ELT business grew and there are so many services to automatically move the data from various online applications to the data warehouse. But there are so few resources and services available to automate data publishing, not to mention no-code solutions.</p><p>Lack of a no-code solution does not stop a business from taking advantage of the powerful ELTP process. A little bit of data engineering investment will work as great leverage on the entire business operation.</p><p>One of the popular Open Source ELT frameworks is <a href="https://singer.io" target="_blank" rel="noopener">singer.io</a>. Singer.io community builds data extractors called tap and data loader called target. Singer.io‚Äôs specification helps the data engineers to mix-and-match tap and target to create the source-destination combo for each business use case. In a typical ELT framework, cloud applications such as Salesforce and Mercato are the data source (tap) and the data warehouses are the destination (target).</p><p>When we built P of ELTP, we reversed the designation: For example, we developed <a href="https://github.com/anelendata/tap-bigquery" target="_blank" rel="noopener">a tap program for BigQuery</a> to extract product usage metrics and developed <a href="https://github.com/anelendata/target-pardot" target="_blank" rel="noopener">a target program for Pardot</a>. By running this tap-target combination, we automated the process of publishing the product usage data from BigQuery to Pardot so that the marketing and sales team of our clients can fully utilize the PQL metrics with no manual work of moving data around.</p><h2 id="Future-of-ELTP"><a href="#Future-of-ELTP" class="headerlink" title="Future of ELTP"></a>Future of ELTP</h2><p>Data publishing is not limited to human consumption. The computed metrics can be replicated back to the production data store or caching layer to enhance the product‚Äôs user experience more optimized and personalized. The metrics could be based on simple statistics or a result of more complex computation by machine-learning. Just by taking care of the last-mile problem and ensure the valuable signal is delivered right where it matters, we can unlock the unseen potential.</p><p>In the near future, would expect there will be more new businesses providing no-code solutions and services to close the loop. Until more people realize how powerful it is, we will help businesses with our custom ELTP solution and make more success stories in sales, marketing, and customer success use cases.</p><h2 id="A-Fun-Demonstration"><a href="#A-Fun-Demonstration" class="headerlink" title="A Fun Demonstration"></a>A Fun Demonstration</h2><p>Here is a fun demonstration of solving the ‚Äúlast-mile problem‚Äù of making use of data. These GIF animations are created and posted on Twitter automatically at specified intervals. We extract the data from the sources (geological and financial), transform the data (including the part to produce the animation), and deliver it to where it matters (social media).</p><p><a href="https://twitter.com/AnelenData/status/1359405517915971586" target="_blank" rel="noopener"><img src="/images/usgs-twitter.gif" alt="usgs"></a></p><p><a href="https://twitter.com/AnelenData/status/1358869327185924106" target="_blank" rel="noopener"><img src="/images/stock-twitter.gif" alt="usgs"></a></p><h3 id="About-the-author"><a href="#About-the-author" class="headerlink" title="About the author"></a>About the author</h3><p>Daigo Tanaka is the CEO and data scientist at ANELEN Co., LLC, a boutique consulting firm focused on data engineering, analytics, and data science. ANELEN‚Äôs mission is to help innovative businesses make smarter decisions with data science.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/images/iStock-1150767851.jpg&quot; alt=&quot;data, where you need&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Solve the ‚Äúdata last-mile problem‚Äù to unlock the full ana
      
    
    </summary>
    
    <content src="http://handoff.cloud//images/iStock-1150767851.jpg" type="image" />
    
    
    
      <category term="ELT" scheme="http://handoff.cloud/tags/ELT/"/>
    
      <category term="ETL" scheme="http://handoff.cloud/tags/ETL/"/>
    
      <category term="ELTP" scheme="http://handoff.cloud/tags/ELTP/"/>
    
      <category term="data engineering" scheme="http://handoff.cloud/tags/data-engineering/"/>
    
      <category term="analytics" scheme="http://handoff.cloud/tags/analytics/"/>
    
  </entry>
  
  <entry>
    <title>Go schemaless: ELT with Google Cloud Storage and BigQuery</title>
    <link href="http://handoff.cloud/elt-google-cloud-storage-bigquery/"/>
    <id>http://handoff.cloud/elt-google-cloud-storage-bigquery/</id>
    <published>2020-06-25T13:12:36.000Z</published>
    <updated>2022-02-10T05:57:34.884Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/earthquakes.png" alt="USGS earthquake events map"></p><p>The image above, generated with the data in this article,<br>is a map showing the location and magnitude (bubble size) of earthquakes on June 24, 2020, based on the USGS Earthquake event data.</p><h2 id="Load-first-Worry-about-schema-later"><a href="#Load-first-Worry-about-schema-later" class="headerlink" title="Load first. Worry about schema later."></a>Load first. Worry about schema later.</h2><p>When we load the data to a data warehouse, we usually<br><a href="https://cloud.google.com/bigquery/docs/schemas" target="_blank" rel="noopener">specify the schema</a><br>upfront.<br>What if we can first load the data without worrying about the schema,<br>and format the data on the data warehouse?</p><p>In a typical ETL (Extract, Transform, and Load) framework, the data is</p><ol><li>Extracted from the source.</li><li>Transformed. The transformation includes formatting to a pre-defined schema.</li><li>Loaded to a data store such as a data warehouse.</li></ol><p>The advantages of doing ELT (Extract, Load, and Transform) instead of include:</p><ol><li>The extraction process won‚Äôt have to change when the schema changes.</li><li>The transformation process can take advantage of the massively-parallel execution by the modern data warehouse.</li></ol><p>In this post, I would like to demonstrate the business impact, especially with speed, when we adopt ELT approach.</p><h2 id="ELT-with-BigQuery-and-Cloud-Storage"><a href="#ELT-with-BigQuery-and-Cloud-Storage" class="headerlink" title="ELT with BigQuery and Cloud Storage"></a>ELT with BigQuery and Cloud Storage</h2><p>In Google Cloud Platform, it is very easy to do ELT with<br><a href="https://cloud.netapp.com/ma-gcp-plp-premium-google-cloud-storage" target="_blank" rel="noopener">Cloud Storage</a><br>and <a href="https://cloud.google.com/bigquery" target="_blank" rel="noopener">BigQuery</a>. Using GCS and BigQuery, Felipe Hoffa demonstrated a lazy-loading of half a trillion Wikipedia page views.<br>(His articles can be found <a href="https://cloud.google.com/blog/products/gcp/bigquery-lazy-data-loading-sql-data-languages-ddl-and-dml-partitions-and-half-a-trillion-wikipedia-pageviews" target="_blank" rel="noopener">here</a> and <a href="https://medium.com/google-cloud/bigquery-lazy-data-loading-ddl-dml-partitions-and-half-a-trillion-wikipedia-pageviews-cd3eacd657b6" target="_blank" rel="noopener">here</a>)</p><p>In his post, Hoffa loaded the super large set of files with a simple format (space-separated) parsing with <code>REGEXP_EXTRACT</code> function.<br>I will use<br><a href="https://earthquake.usgs.gov/fdsnws/event/1/" target="_blank" rel="noopener">the USGS Earthquake event data</a> as an example dataset. The earthquake event data is a newline-separated JSON file that looks like this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;type&quot;:&quot;Feature&quot;,</span><br><span class="line">  &quot;properties&quot;:&#123;</span><br><span class="line">    &quot;mag&quot;:1.1000000000000001,</span><br><span class="line">    &quot;place&quot;:&quot;117km NW of Talkeetna, Alaska&quot;,</span><br><span class="line">    &quot;time&quot;:1388620046501,</span><br><span class="line">    &quot;updated&quot;:1558392330681,</span><br><span class="line">    &quot;tz&quot;:-540,</span><br><span class="line">    ...</span><br><span class="line">    &quot;type&quot;:&quot;earthquake&quot;,</span><br><span class="line">    &quot;title&quot;:&quot;M 1.1 - 117km NW of Talkeetna, Alaska&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;geometry&quot;:&#123;</span><br><span class="line">    &quot;type&quot;:&quot;Point&quot;,</span><br><span class="line">    &quot;coordinates&quot;:[</span><br><span class="line">      -151.64590000000001,</span><br><span class="line">      63.101999999999997,</span><br><span class="line">      14.1</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;id&quot;:&quot;ak01421ig3u&quot;</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">  &quot;type&quot;:&quot;Feature&quot;,</span><br><span class="line">  &quot;properties&quot;:&#123;</span><br><span class="line">    &quot;mag&quot;:1.2,</span><br><span class="line">    &quot;place&quot;:&quot;6km SSW of Big Lake, Alaska&quot;,</span><br><span class="line">    &quot;time&quot;:1388619956476,</span><br><span class="line">    &quot;updated&quot;:1558392330249,</span><br><span class="line">    &quot;tz&quot;:-540,</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    &quot;type&quot;:&quot;earthquake&quot;,</span><br><span class="line">    &quot;title&quot;:&quot;M 1.2 - 6km SSW of Big Lake, Alaska&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;geometry&quot;:&#123;</span><br><span class="line">    &quot;type&quot;:&quot;Point&quot;,</span><br><span class="line">    &quot;coordinates&quot;:[</span><br><span class="line">      -150.01650000000001,</span><br><span class="line">      61.458100000000002,</span><br><span class="line">      44.600000000000001</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;id&quot;:&quot;ak01421i2zj&quot;</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><h2 id="stdin-to-GCS"><a href="#stdin-to-GCS" class="headerlink" title="stdin to GCS"></a>stdin to GCS</h2><p>I wrote <a href="https://github.com/anelendata/target_gcs" target="_blank" rel="noopener">target_gcs</a>,<br>a Python program that takes stdin and writes out to a Cloud Storage location. It is easy to install via pip:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python3 -m venv .&#x2F;venv</span><br><span class="line">source .&#x2F;venv&#x2F;bin&#x2F;activate</span><br><span class="line">pip install https:&#x2F;&#x2F;github.com&#x2F;anelendata&#x2F;target_gcs&#x2F;tarball&#x2F;master</span><br></pre></td></tr></table></figure><p>Then follow the <a href="https://github.com/anelendata/target_gcs/blob/master/README.md#configure" target="_blank" rel="noopener">instruction</a> to configure the Google Cloud Platform project and data location.</p><p>After the simple set up, you can run</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl &quot;https:&#x2F;&#x2F;earthquake.usgs.gov&#x2F;fdsnws&#x2F;event&#x2F;1&#x2F;query?format&#x3D;geojson&amp;starttime&#x3D;2020-06-24&amp;endtime&#x3D;2020-06-25&quot; | \</span><br><span class="line">target_gcs -c .&#x2F;your-config.json</span><br></pre></td></tr></table></figure><p>to get the data loaded on Cloud Storage.<br>In this simple example, the USGS data is fetched by <code>curl</code> command,<br>and the data is written out to stdout.<br>target_gcs receives the data through pipe (<code>|</code>) and writes out to GCS.</p><h2 id="GCS-to-BigQuery"><a href="#GCS-to-BigQuery" class="headerlink" title="GCS to BigQuery"></a>GCS to BigQuery</h2><p>After the data is loaded to Cloud Storage, it‚Äôs time to create a table on BigQuery.<br>For this, we can create an <a href="https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs" target="_blank" rel="noopener">externally partitioned table</a><br>with the <a href="https://github.com/anelendata/target_gcs/blob/master/create_schemaless_table.py" target="_blank" rel="noopener">Python script</a> I provided (*):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python create_schemaless_table.py \</span><br><span class="line">  -p your-project-id \</span><br><span class="line">  -g gs:&#x2F;&#x2F;your-bucket&#x2F;your-dataset \</span><br><span class="line">  -d your-dataset-name \</span><br><span class="line">  -t your-table-name</span><br></pre></td></tr></table></figure><p>In my case, I set the target dataset name to <code>gcs_partitioned</code><br>and table name to <code>usgs_events_unparsed</code>.</p><p>(*) Note: If the JSON schema in the data is stable and consistent, it makes more sense to load in<br><a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json" target="_blank" rel="noopener">the standard way of loading JSON data</a>.<br>In the method in this article is advantageous when you have JSON record whose set of keys are inconsistent among records.<br>I also had a case where I needed to first search for all the keys present in the data, then flatten/unwrap/unpivot the data into a long format.</p><h2 id="Unstructured-data-on-BigQuery"><a href="#Unstructured-data-on-BigQuery" class="headerlink" title="Unstructured data on BigQuery"></a>Unstructured data on BigQuery</h2><p>Let‚Äôs query the unstructured data.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT *</span><br><span class="line">FROM &#96;gcs_partitioned.usgs_events_unparsed&#96;</span><br><span class="line">WHERE date BETWEEN DATE &#39;2020-06-24&#39; AND DATE &#39;2020-06-25&#39; LIMIT 10</span><br></pre></td></tr></table></figure><p><img src="/images/usgs_unparsed_sample_bigquery.png" alt=""></p><p>You can see <code>line</code> as the raw JSON string. You also see columns such as <code>version</code>, <code>format</code>, and <code>date</code>.<br>They are from the GCS file partition information. By using those keys, we can limit the total data scanned with a query.<br>In fact, the Python script we used to create the BigQuery table from GCS set the option to always require the partition filter in the query.<br>This is a good practice to avoid running a costly query by accident. (BigQuery costs $5 per TB data scanned.)</p><h2 id="Extract-the-fields-from-JSON-to-create-a-structured-view"><a href="#Extract-the-fields-from-JSON-to-create-a-structured-view" class="headerlink" title="Extract the fields from JSON to create a structured view"></a>Extract the fields from JSON to create a structured view</h2><p>We can create a view that has the structure from the unstructured table:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">CREATE VIEW &#96;views.usgs_events&#96; AS (</span><br><span class="line">  SELECT</span><br><span class="line">    STRUCT(</span><br><span class="line">      TIMESTAMP_MILLIS(</span><br><span class="line">         CAST(JSON_EXTRACT(line,</span><br><span class="line">          &#39;$.properties.time&#39;) AS INT64)) AS time,</span><br><span class="line">      CAST(JSON_EXTRACT(line,</span><br><span class="line">          &#39;$.properties.tz&#39;) AS INT64) AS tz,</span><br><span class="line">      CAST(JSON_EXTRACT(line,</span><br><span class="line">          &#39;$.properties.mag&#39;) AS FLOAT64) AS mag,</span><br><span class="line">      TRIM(JSON_EXTRACT(line,</span><br><span class="line">          &#39;$.properties.place&#39;)) AS place ) AS properties,</span><br><span class="line">      CONCAT(JSON_EXTRACT_ARRAY(line, &#39;$.geometry.coordinates&#39;)[OFFSET(1)], &#39;,&#39;,</span><br><span class="line">             JSON_EXTRACT_ARRAY(line, &#39;$.geometry.coordinates&#39;)[OFFSET(0)]) AS lat_lng,</span><br><span class="line">    date AS etl_date</span><br><span class="line">  FROM &#96;gcs_partitioned.usgs_events_unparsed&#96;</span><br><span class="line">  WHERE JSON_EXTRACT(line, &#39;$.type&#39;) &#x3D; &#39;&quot;Feature&quot;&#39;</span><br><span class="line">  )</span><br></pre></td></tr></table></figure><p>Here, I‚Äôm using functions such as <code>JSON_EXTRACT</code> and <code>JSON_EXTRACT_ARRAY</code> to extract the values from the nested field to create the flat table.<br>I‚Äôm using <code>CAST</code> function to convert the extracted string into the appropriate data types:</p><p><img src="/images/usgs_events_view.png" alt=""></p><p>Note that latitude and longitude are concatenated as a comma-separated string for the next step.</p><p>Also note that I renamed <code>date</code> partition field to <code>etl_date</code>.<br>The view inherits the partition filter requirement so we can avoid any accident of scanning the entire dataset by accident:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM &#96;views.usgs_events&#96;</span><br><span class="line">WHERE etl_date BETWEEN DATE &#39;2020-06-24&#39; and DATE &#39;2020-06-25&#39;</span><br></pre></td></tr></table></figure><p><img src="/images/usgs_parsed_events_sample.png" alt=""></p><p>Once the data is formatted, it is straightforward to visualize it on Google Data Studio:</p><p><img src="/images/usgs_events_map.png" alt=""></p><p>Here, Google Maps are selected as the chart type.<br><code>lag_lng</code> is used for bubble location,<br>and <code>property.mag</code> is used as the bubble size.</p><h2 id="ELT-Simple-and-powerful"><a href="#ELT-Simple-and-powerful" class="headerlink" title="ELT: Simple and powerful"></a>ELT: Simple and powerful</h2><p>In this post, I extracted the data simply with curl command without worrying about the file format or schema. The data was just dumped to Cloud Storage.<br>Then I created an externally partitioned table. Here again, I did not care to specify a schema.<br>At the very last step, I extracted the values from the raw strings and saved them as a structured view. I did so just by writing a SQL.<br>I could extract what I want with <code>JSON_EXTRACT</code> function for newline-separated JSON records. <code>REGEXP_EXTRACT</code> function would be useful for other types of file formats.</p><p>The parallelization of the query execution comes free, thanks to BigQuery.<br>Just imagine how slow the process would be if I had to</p><ol><li>Specify the schema.</li><li>Write the transformation logic.</li><li>Configure the parallel processing for the transformation process by myself.</li></ol><p>‚Ä¶before loading the data to the data warehouse.<br>What if the raw data‚Äôs schema suddenly changed?</p><p>I hope you are impressed by how quickly I processed and visualized the earthquake event data by choosing the simple and powerful ETL approach.<br>The computational benefit of transforming data in a massively-parallel data warehouse such as BigQuery would be even greater once I start extracting much more data continuously with a production-level extraction process such as <a href="https://articles.anelen.co/kinoko-io-etl/" target="_blank" rel="noopener">this one</a>.</p><h2 id="A-word-of-caution"><a href="#A-word-of-caution" class="headerlink" title="A word of caution"></a>A word of caution</h2><p>I just demonstrated a case of efficiency gain with ELT.<br>However, ETL or ETLT can be more appropriate in other scenarios.<br>For example, masking or obfuscating personal identifiable data is very important before storing them in a data lake including Cloud Storage because it is much more costly to delete or modify a single record in a file partition after it‚Äôs loaded. (Think of a GDPR case a user requests a complete data erase.)</p><h2 id="Extra-Animate-the-earthquake-events-on-the-map"><a href="#Extra-Animate-the-earthquake-events-on-the-map" class="headerlink" title="Extra: Animate the earthquake events on the map"></a>Extra: Animate the earthquake events on the map</h2><p><img src="/images/earthquakes_20200624.gif" alt="earthquakes"></p><p>Above visualization was created with this <a href="https://colab.research.google.com/drive/1Ajf3VzqPxJpu9Vrq5EZTDRGRxP6HjXC0?usp=sharing" target="_blank" rel="noopener">source code</a></p><p>I hope this introduction was informative and relatable to your business. If you would like to find out more what Anelen can do for your data challenge, please contact <a href="mailto:hello@anelen.co">hello@anelen.co</a> (.co, not .com don‚Äôt get your email lost elsewhere!) or schedule a free discovery meeting with the <a href="https://calendly.com/anelen-discovery" target="_blank" rel="noopener">scheduler</a>.</p><p>Daigo Tanaka, CEO and Data Scientist @ ANELEN</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/images/earthquakes.png&quot; alt=&quot;USGS earthquake events map&quot;&gt;&lt;/p&gt;
&lt;p&gt;The image above, generated with the data in this article,&lt;br&gt;
      
    
    </summary>
    
    <content src="http://handoff.cloud//images/earthquakes.png" type="image" />
    
    
    
  </entry>
  
</feed>
