<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>handoff</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://handoff.cloud/"/>
  <updated>2021-07-29T03:38:00.834Z</updated>
  <id>http://handoff.cloud/</id>
  
  <author>
    <name>handoff</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>handoff founder interview</title>
    <link href="http://handoff.cloud/handoff-founder-interview/"/>
    <id>http://handoff.cloud/handoff-founder-interview/</id>
    <published>2021-07-27T23:56:07.000Z</published>
    <updated>2021-07-29T03:38:00.834Z</updated>
    
    <content type="html"><![CDATA[<p>Discover the advantages of using handoff ELT for data analytics.<br>Daigo Tanaka, founder of handoff.cloud answers everything you need to know about handoff.</p><video width="100%" controls poster="/images/founder-interview-poster.jpg">  <source src="https://handoff.cloud/assets/video/handoff-founder-interview.mp4" type="video/mp4">Your browser does not support the video tag.</video>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Discover the advantages of using handoff ELT for data analytics.&lt;br&gt;Daigo Tanaka, founder of handoff.cloud answers everything you need to
      
    
    </summary>
    
    <content src="http://handoff.cloud//images/founder-interview-poster.jpg" type="image" />
    
    
    
  </entry>
  
  <entry>
    <title>handoff 1 minute introduction video!</title>
    <link href="http://handoff.cloud/handoff-1min-intro-md/"/>
    <id>http://handoff.cloud/handoff-1min-intro-md/</id>
    <published>2021-07-21T23:24:41.000Z</published>
    <updated>2021-07-29T03:38:00.834Z</updated>
    
    <content type="html"><![CDATA[<p>Here is our first ever introduction video of handoff platform and service.<br>Please check out!</p><iframe frameborder="0" width="100%" height="500px" src="https://biteable.com/watch/embed/3087876/ccb9dea7f33aaae9fcec4479d8e11283" allowfullscreen="true" allow="autoplay"> </iframe>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Here is our first ever introduction video of handoff platform and service.&lt;br&gt;Please check out!&lt;/p&gt;
&lt;iframe frameborder=&quot;0&quot; width=&quot;100%&quot; 
      
    
    </summary>
    
    <content src="http://handoff.cloud/https://handoff.cloud/assets/img/handoff-ui.png" type="image" />
    
    
    
  </entry>
  
  <entry>
    <title>handoff on Meltano Demo Day</title>
    <link href="http://handoff.cloud/handoff-on-Meltano-Demo-Day/"/>
    <id>http://handoff.cloud/handoff-on-Meltano-Demo-Day/</id>
    <published>2021-05-21T23:31:55.000Z</published>
    <updated>2021-07-29T03:38:00.834Z</updated>
    
    <content type="html"><![CDATA[<p>handoff’s Daigo Tanaka had the pleasure of sharing a demo of our ETL/ELT platform on Meltano’s Demo Day. If you can spare 10min please check out the video recording:</p><iframe width="100%" height="500px" src="https://www.youtube.com/embed/oHRon2xzeiw?start=1317" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;handoff’s Daigo Tanaka had the pleasure of sharing a demo of our ETL/ELT platform on Meltano’s Demo Day. If you can spare 10min please ch
      
    
    </summary>
    
    <content src="http://handoff.cloud//images/meltano-demo-day.png" type="image" />
    
    
    
  </entry>
  
  <entry>
    <title>Introducing Handoff: Serverless Data Pipeline Orchestration Framework</title>
    <link href="http://handoff.cloud/handoff-Serverless-Data-Pipeline-Orchestration-Framework/"/>
    <id>http://handoff.cloud/handoff-Serverless-Data-Pipeline-Orchestration-Framework/</id>
    <published>2021-03-20T13:47:40.000Z</published>
    <updated>2021-07-29T03:38:00.834Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://articles.anelen.co/images/keep-flowing-handoff.gif" alt="data-flow"></p><p>This article presents the business context and a technical deep dive of our serverless approach to data pipeline deployment. Our approach is <a href="https://github.com/anelendata/handoff" target="_blank" rel="noopener">open-sourced</a> as handoff: framework for serverless data pipeline orchestration.</p><h2 id="Business-Context-Helping-businesses-bring-any-data-anywhere"><a href="#Business-Context-Helping-businesses-bring-any-data-anywhere" class="headerlink" title="Business Context: Helping businesses bring any data anywhere"></a>Business Context: Helping businesses bring any data anywhere</h2><p>More and more companies have started to leverage cloud applications and their data in order to make their operation lean and effective. For example, D2C (direct to consumer) companies produce niche household products and ship directly to their customers instead of relying on a traditional retail distribution channel.</p><p>In doing so, a D2C company may use a marketing automation platform like Marketo or Pardot, run ads on Facebook and Google, and manage the supply chain with Flexport: All of these applications produce customer experience data. It is a common practice to extract data from cloud services and load them into a data warehouse (DWH). By combining data sources and analyzing them together, the business continuously improves the quality of the customer experience.</p><p>We are a technology-leveraged service company offering custom ETL/ELT solutions. ETL stands for Extracting, Transforming, and Loading of data. ELT workflows are becoming popular. Data are extracted and loaded into the data warehouse before transformation is executed by a massively parallel modern DWH engine.</p><p>The demand for implementing data pipelines is growing at an unprecedented pace. Data replication services such as Fivetran and StitchData provide data connectors for popular cloud applications such as Salesforce and Zendesk. However, those companies cannot support the long-tail of cloud applications in the Software as a Service (SaaS) market. If a cloud application is not supported, a business needs a custom data engineering job, but not every company can afford to developing an internal data engineering team.</p><p><img src="https://articles.anelen.co/images/long-tail.png" alt="No cloud apps left behind"></p><p>That is why fully-managed ETL services like ours are becoming the new option: With a fraction of the cost of hiring a data engineer, businesses can install the tailor-made data pipeline service.</p><p>While we serve our customers with active monitoring and maintenance, we decided to open-source the core framework as a contribution to data engineering community. In the rest of the article, we will share the technical solution that made our service possible.</p><h2 id="Technical-challenges-and-solution"><a href="#Technical-challenges-and-solution" class="headerlink" title="Technical challenges and solution"></a>Technical challenges and solution</h2><p>If you are the director of data engineering for your company’s internal operations, it makes sense to set up an Apache Airflow or subscribe to a managed service by Astronomer or Cloud Composer by Google Cloud Platform. Instead, we are a lean team helping others to perform the data integration of cloud applications in the long tail.</p><p>Simultaneously serving the data pipelines for many different clients change the technical requirements a lot. The most important part is to provide separate CPU, memory, and storage capacities for each client. We wanted to achieve this with maximum flexibility. When we started we only had a few data pipelines to manage, but we expected the number to rapidly grow.</p><h2 id="AWS-Lambda-vs-Fargate"><a href="#AWS-Lambda-vs-Fargate" class="headerlink" title="AWS Lambda vs. Fargate"></a>AWS Lambda vs. Fargate</h2><p>We went for a serverless approach. Our first prototype used Amazon Web Services (AWS) Lambda. It was quick to implement. The pay-as-you-go pricing was very attractive. We didn’t have to pay for idling the virtual machine or deal with peak-time auto-scaling challenges. But we soon realized that Lambda’s 15 minutes processing time limit is not enough for some tasks. So we switched to Fargate, another pay-as-you-go serverless platform by AWS. It has no time limit and the life-cycle management and scaling of computing resource is fully managed by the platform.</p><p>Compared to Lambda, Fargate’s deployment process is much more complex. Lambda could be deployed just by writing a script file. Fargate deployment involves Docker image creation, pushing the image to Elastic Container Registry, creating resources such as Virtual Private Cloud and Security Groups, defining the Fargate task, managing the Policy, and so on.</p><h2 id="Birth-of-handoff-A-serverless-data-pipeline-orchestration-framework"><a href="#Birth-of-handoff-A-serverless-data-pipeline-orchestration-framework" class="headerlink" title="Birth of handoff: A serverless data pipeline orchestration framework"></a>Birth of handoff: A serverless data pipeline orchestration framework</h2><p>The painful Fargate deployment process gave us the idea to implement a command-line interface (CLI) that we later named “handoff”. handoff simplifies the deployment steps for AWS Fargate. We may also provide a multi-cloud capability in the future: handoff unifies the commands to deploy to AWS, Microsoft Azure, Google Cloud Platform, and so on.</p><p>handoff describes the data pipeline logic in a YAML file. The first thing is to describe the installation of the modules required by the pipeline logic:</p><p>project.yml:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">version: 0.3</span><br><span class="line">description: Fetch foreign exchange rates</span><br><span class="line"></span><br><span class="line">installs:</span><br><span class="line">- venv: extractor</span><br><span class="line">  command: pip install tap-exchangeratesapi</span><br><span class="line">- venv: loader</span><br><span class="line">  command: pip install target-bigquery-partition</span><br></pre></td></tr></table></figure><p>As you can see, handoff is created primarily for Python. You can install modules in separate virtual environments if needed: handoff automatically generates multiple virtual environments. In local testing, you can install modules by running:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">handoff workspace install --project &lt;project_dir&gt; --workspace &lt;workspace_dir&gt;</span><br></pre></td></tr></table></figure><p>The pipeline logic is described as groups of tasks. In a task group, each command can be linked as a Unix pipeline:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tasks:</span><br><span class="line">- name: fetch_exchange_rates</span><br><span class="line">  description: Fetch exchange rates and load on BigQuery</span><br><span class="line">  pipeline:</span><br><span class="line">  - command: tap-exchangeratesapi</span><br><span class="line">    args: --config files&#x2F;tap-config.json</span><br><span class="line">    venv: extractor</span><br><span class="line">  - command: &quot;target-bigquery --config files&#x2F;target_config.json&quot;</span><br><span class="line">    venv: loader</span><br></pre></td></tr></table></figure><p>The pipeline is particularly useful in deploying memory-efficient tasks such as the ELT task by singer.io framework.</p><p>The following command runs the task locally:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">handoff run local -p &lt;project_dir&gt; -w &lt;workspace_dir&gt;</span><br></pre></td></tr></table></figure><h2 id="Doing-more-than-a-simple-data-replication-task"><a href="#Doing-more-than-a-simple-data-replication-task" class="headerlink" title="Doing more than a simple data replication task"></a>Doing more than a simple data replication task</h2><p>handoff also implements simple control flows such as for-each and fork. You can also define a task beyond simple ETL. Look how we automate the task of fetching stock market data, make a GIF animation, and post it on Twitter:</p><blockquote class="twitter-tweet"><p lang="en" dir="ltr">A fun demo: handoff&#39;s <a href="https://twitter.com/hashtag/datapipeline?src=hash&amp;ref_src=twsrc%5Etfw" target="_blank" rel="noopener">#datapipeline</a> can automatically harvest data and post data-driven content to Twitter <a href="https://twitter.com/hashtag/etl?src=hash&amp;ref_src=twsrc%5Etfw" target="_blank" rel="noopener">#etl</a> <a href="https://twitter.com/hashtag/analytics?src=hash&amp;ref_src=twsrc%5Etfw" target="_blank" rel="noopener">#analytics</a>. <a href="https://t.co/0ITR771mh9" target="_blank" rel="noopener">pic.twitter.com/0ITR771mh9</a></p>&mdash; handoff (@handoffcloud) <a href="https://twitter.com/handoffcloud/status/1377477316448636935?ref_src=twsrc%5Etfw" target="_blank" rel="noopener">April 1, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script><h2 id="Deployment-features"><a href="#Deployment-features" class="headerlink" title="Deployment features"></a>Deployment features</h2><p>We estimate authoring pipeline logic is just 20% of the ETL development. To run the pipeline in production, we need to implement management of:</p><ul><li>Run-time configuration</li><li>Secret keys</li><li>Scheduling</li><li>Logging</li><li>Monitoring</li><li>Containerization<br>…and the list goes on. handoff’s project.yml helps to keep those organized and it is intuitive to understand. Here is an example of project.yml:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">vars:</span><br><span class="line">- key: gcp_project_id</span><br><span class="line">  value: &quot;handoff-demo&quot;</span><br><span class="line">- key: dataset_id</span><br><span class="line">  value: exchangerate</span><br><span class="line"></span><br><span class="line">envs:</span><br><span class="line">- key: GOOGLE_APPLICATION_CREDENTIALS</span><br><span class="line">  value: files&#x2F;google_client_secret.json</span><br><span class="line"></span><br><span class="line">deploy:</span><br><span class="line">  cloud_provider: aws</span><br><span class="line">  cloud_platform: fargate</span><br><span class="line">  resource_group: handoff-etl</span><br><span class="line">  container_image: tap-rest-api-target-bigquery</span><br><span class="line">  task: usgs-earthquakes</span><br><span class="line"></span><br><span class="line">schedules:</span><br><span class="line">- target_id: 1</span><br><span class="line">  description: Run everyday at 00:00:00Z</span><br><span class="line">  envs:</span><br><span class="line">  - key: __VARS</span><br><span class="line">    value: &#39;start_datetime&#x3D;$(date -Iseconds -d &quot;00:00 yesterday&quot;) end_datetime&#x3D;$(date -Iseconds -d &quot;00:00 today&quot;)&#39;</span><br><span class="line">  cron: &#39;0 0 * * ? *&#39;</span><br></pre></td></tr></table></figure><p>The commands to execute the containerization and the deployment are also simple and intuitive:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Push project configuration, supporting files, and secrets to AWS</span><br><span class="line">handoff project push -p &lt;project_dir&gt;</span><br><span class="line"></span><br><span class="line"># Build a Docker image and push to ECR</span><br><span class="line">handoff container build -p &lt;project_dir&gt;</span><br><span class="line">handoff container push -p &lt;project_dir&gt;</span><br><span class="line"></span><br><span class="line"># Create resource group, bucket, task...</span><br><span class="line">handoff cloud bucket create -p &lt;project_dir&gt;</span><br><span class="line">handoff cloud resources create -p &lt;project_dir&gt;</span><br><span class="line">handoff cloud task create -p &lt;project_dir&gt;</span><br><span class="line"></span><br><span class="line"># Schedule a task</span><br><span class="line">handoff cloud schedule create -p &lt;project_dir&gt;</span><br></pre></td></tr></table></figure><p>handoff simplifies development and deployment. Without handoff, you would have to author CloudFormation templates, deal with boto3 commands to transfer files to S3, figure out a way to encrypt and store the secrets, configure the CloudWatch for logging and metrics, etc.</p><p>For a full handoff tutorial, please visit <a href="https://dev.handoff.cloud" target="_blank" rel="noopener">https://dev.handoff.cloud</a></p><p>We made handoff <a href="https://github.com/anelendata/handoff" target="_blank" rel="noopener">a public project on GitHub</a> so every engineer having a similar challenge can use it. We also welcome collaborators to improve this new framework.</p><p><a href="https://articles.anelen.co/images/this_is_handoff.png" target="_blank" rel="noopener">handoff open source</a></p><h2 id="What’s-next"><a href="#What’s-next" class="headerlink" title="What’s next?"></a>What’s next?</h2><p>As we mentioned earlier, we may support deployment to other cloud platforms such as Microsoft Azure and Google Cloud Platform. Unifying the deployment commands for multi-cloud would be very useful in quickly switching platforms.</p><p>We also prototyped a web UI built on top of the CLI:</p><p><a href="https://articles.anelen.co/images/handoff-ui.gif" target="_blank" rel="noopener">dashboard</a></p><p>The web UI could be running as a stand-alone application on your local machine. It will spin up a local web server. You can interact with handoff via the web UI and the backend connects to cloud resources. It can also be hosted on a server for enterprise use cases.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>AWS Fargate lets us create a cost-effective and scalable solution for ETL deployment. However, it hasn’t been a popular approach due to the complex deployment process. handoff, a serverless data pipeline orchestration framework simplifies it. handoff also manages the various aspects of the orchestration such as security and monitoring. We made handoff an open-source project so that the data engineering community can benefit from it and collaborate for further innovation.</p><h2 id="About-the-author"><a href="#About-the-author" class="headerlink" title="About the author"></a>About the author</h2><p><a href="https://www.linkedin.com/in/daigotanaka/" target="_blank" rel="noopener">Daigo Tanaka</a> is CEO and data scientist at <a href="https://anelen.co/" target="_blank" rel="noopener">ANELEN</a>, a boutique consulting firm focused on data engineering, analytics, and data science. ANELEN’s mission is to help innovative businesses make smarter decisions with data science. ANELEN is the provider of handoff.could ETL/ELT service.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://articles.anelen.co/images/keep-flowing-handoff.gif&quot; alt=&quot;data-flow&quot;&gt;&lt;/p&gt;
&lt;p&gt;This article presents the business context 
      
    
    </summary>
    
    <content src="http://handoff.cloud/https://articles.anelen.co/images/keep-flowing-handoff.gif" type="image" />
    
    
    
  </entry>
  
  <entry>
    <title>What is P in ELTP Process?</title>
    <link href="http://handoff.cloud/What-is-P-in-ELTP-Process/"/>
    <id>http://handoff.cloud/What-is-P-in-ELTP-Process/</id>
    <published>2021-02-12T17:53:30.000Z</published>
    <updated>2021-07-29T03:38:00.834Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/iStock-1150767851.jpg" alt="data, where you need"></p><p><em>Solve the “data last-mile problem” to unlock the full analytics capabilities for your business.</em></p><h2 id="Adding-P-to-ELT"><a href="#Adding-P-to-ELT" class="headerlink" title="Adding P to ELT"></a>Adding P to ELT</h2><p>If you worked with a data-engineer, you may have heard the word ETL or ELT.</p><ul><li>E for Extracting data from the source.</li><li>T for Transforming from raw data to clean and useful information.</li><li>L for Loading to the destination to a data lake or data warehouse.</li></ul><p>The trend is to transform the data after loading the data to the powerful modern data warehouse such as Google BigQuery, Snowflake, AWS Redshift, and Microsoft Synapse. So, ELT rather than ETL is increasingly used to refer the data processing.</p><p>I’m adding P to ELT, and it’s “Publishing”. It’s the process of publishing the post-transformation data to exactly where people or another program consume them. We call a “frontline application” in this article.</p><h2 id="ELT-does-not-solve-the-“Data-Last-mile-Problem”"><a href="#ELT-does-not-solve-the-“Data-Last-mile-Problem”" class="headerlink" title="ELT does not solve the “Data Last-mile Problem”"></a>ELT does not solve the “Data Last-mile Problem”</h2><p>Let me expand this by a realistic business scenario: Product Qualified Lead scoring (PQL scoring).</p><p>Suppose your company is a Software-as-a-Service (SaaS) company. The service is free to sign up to use the basic feature. Your potential customer finds your service through an online ad. Sign up is free but the user must enter their work email and business information.</p><p>If you are using online marketing automation tools such as Marketo or Pardot, you can capture the prospective customer’s product discovery channel and their email at this point. But the marketing process typically lacks information on the usage statistics of your service. It’s because such data is typically sitting on the production database.</p><p>Or you may have taken one step further to replicate the data from the production database to the data warehouse through an ELT process. You may be a data scientist who came up with a formula or machine learning algorithm to compute a PQL score to indicate which free-tier users would likely convert to paying customers.</p><p>But as long as the data is sitting in the warehouse, it’s not going to be utilized. In the case of the PQL score like above, such score should be published to Marketo, Pardot, or Salesforce because that is where the sales and marketing staff do their job. They are too busy to open a business intelligence tool or run queries to find which prospects should be prioritized.</p><p><img src="/images/eltp.png" alt="ELTP process"></p><h2 id="Publish-Push-the-data-out-of-the-warehouse"><a href="#Publish-Push-the-data-out-of-the-warehouse" class="headerlink" title="Publish: Push the data out of the warehouse"></a>Publish: Push the data out of the warehouse</h2><p>The importance of publishing the metrics to the frontline applications is critical beyond the product marketing use cases. Another compelling case for SaaS business is customer success. For a subscription-based service, it is crucial to track the health of each subscriber account. Especially for complex business applications, the customer may give up on the product before seeing the value. Are your customers taking the right steps towards the value after signing up, or are they having difficulties starting out?</p><p>Enterprise SaaS companies typically have a customer success function to help the new account in the onboarding process and beyond. The product usage statistics and account’s health score would be very helpful only if such information is available right where they do their work such as Zendesk.</p><p>ELTP’s P takes care of the last mile problem of information delivery to make the business operation smart, efficient, and lean.</p><h2 id="ELTP-Automation"><a href="#ELTP-Automation" class="headerlink" title="ELTP Automation"></a>ELTP Automation</h2><p>Over the years, the ELT business grew and there are so many services to automatically move the data from various online applications to the data warehouse. But there are so few resources and services available to automate data publishing, not to mention no-code solutions.</p><p>Lack of a no-code solution does not stop a business from taking advantage of the powerful ELTP process. A little bit of data engineering investment will work as great leverage on the entire business operation.</p><p>One of the popular Open Source ELT frameworks is <a href="https://singer.io" target="_blank" rel="noopener">singer.io</a>. Singer.io community builds data extractors called tap and data loader called target. Singer.io’s specification helps the data engineers to mix-and-match tap and target to create the source-destination combo for each business use case. In a typical ELT framework, cloud applications such as Salesforce and Mercato are the data source (tap) and the data warehouses are the destination (target).</p><p>When we built P of ELTP, we reversed the designation: For example, we developed <a href="https://github.com/anelendata/tap-bigquery" target="_blank" rel="noopener">a tap program for BigQuery</a> to extract product usage metrics and developed <a href="https://github.com/anelendata/target-pardot" target="_blank" rel="noopener">a target program for Pardot</a>. By running this tap-target combination, we automated the process of publishing the product usage data from BigQuery to Pardot so that the marketing and sales team of our clients can fully utilize the PQL metrics with no manual work of moving data around.</p><h2 id="Future-of-ELTP"><a href="#Future-of-ELTP" class="headerlink" title="Future of ELTP"></a>Future of ELTP</h2><p>Data publishing is not limited to human consumption. The computed metrics can be replicated back to the production data store or caching layer to enhance the product’s user experience more optimized and personalized. The metrics could be based on simple statistics or a result of more complex computation by machine-learning. Just by taking care of the last-mile problem and ensure the valuable signal is delivered right where it matters, we can unlock the unseen potential.</p><p>In the near future, would expect there will be more new businesses providing no-code solutions and services to close the loop. Until more people realize how powerful it is, we will help businesses with our custom ELTP solution and make more success stories in sales, marketing, and customer success use cases.</p><h2 id="A-Fun-Demonstration"><a href="#A-Fun-Demonstration" class="headerlink" title="A Fun Demonstration"></a>A Fun Demonstration</h2><p>Here is a fun demonstration of solving the “last-mile problem” of making use of data. These GIF animations are created and posted on Twitter automatically at specified intervals. We extract the data from the sources (geological and financial), transform the data (including the part to produce the animation), and deliver it to where it matters (social media).</p><p><a href="https://twitter.com/AnelenData/status/1359405517915971586" target="_blank" rel="noopener"><img src="/images/usgs-twitter.gif" alt="usgs"></a></p><p><a href="https://twitter.com/AnelenData/status/1358869327185924106" target="_blank" rel="noopener"><img src="/images/stock-twitter.gif" alt="usgs"></a></p><h3 id="About-the-author"><a href="#About-the-author" class="headerlink" title="About the author"></a>About the author</h3><p>Daigo Tanaka is the CEO and data scientist at ANELEN Co., LLC, a boutique consulting firm focused on data engineering, analytics, and data science. ANELEN’s mission is to help innovative businesses make smarter decisions with data science.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/images/iStock-1150767851.jpg&quot; alt=&quot;data, where you need&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Solve the “data last-mile problem” to unlock the full ana
      
    
    </summary>
    
    <content src="http://handoff.cloud//images/iStock-1150767851.jpg" type="image" />
    
    
    
      <category term="ELT" scheme="http://handoff.cloud/tags/ELT/"/>
    
      <category term="ETL" scheme="http://handoff.cloud/tags/ETL/"/>
    
      <category term="ELTP" scheme="http://handoff.cloud/tags/ELTP/"/>
    
      <category term="data engineering" scheme="http://handoff.cloud/tags/data-engineering/"/>
    
      <category term="analytics" scheme="http://handoff.cloud/tags/analytics/"/>
    
  </entry>
  
  <entry>
    <title>Go schemaless: ELT with Google Cloud Storage and BigQuery</title>
    <link href="http://handoff.cloud/elt-google-cloud-storage-bigquery/"/>
    <id>http://handoff.cloud/elt-google-cloud-storage-bigquery/</id>
    <published>2020-06-25T13:12:36.000Z</published>
    <updated>2021-07-29T03:38:00.834Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/earthquakes.png" alt="USGS earthquake events map"></p><p>The image above, generated with the data in this article,<br>is a map showing the location and magnitude (bubble size) of earthquakes on June 24, 2020, based on the USGS Earthquake event data.</p><h2 id="Load-first-Worry-about-schema-later"><a href="#Load-first-Worry-about-schema-later" class="headerlink" title="Load first. Worry about schema later."></a>Load first. Worry about schema later.</h2><p>When we load the data to a data warehouse, we usually<br><a href="https://cloud.google.com/bigquery/docs/schemas" target="_blank" rel="noopener">specify the schema</a><br>upfront.<br>What if we can first load the data without worrying about the schema,<br>and format the data on the data warehouse?</p><p>In a typical ETL (Extract, Transform, and Load) framework, the data is</p><ol><li>Extracted from the source.</li><li>Transformed. The transformation includes formatting to a pre-defined schema.</li><li>Loaded to a data store such as a data warehouse.</li></ol><p>The advantages of doing ELT (Extract, Load, and Transform) instead of include:</p><ol><li>The extraction process won’t have to change when the schema changes.</li><li>The transformation process can take advantage of the massively-parallel execution by the modern data warehouse.</li></ol><p>In this post, I would like to demonstrate the business impact, especially with speed, when we adopt ELT approach.</p><h2 id="ELT-with-BigQuery-and-Cloud-Storage"><a href="#ELT-with-BigQuery-and-Cloud-Storage" class="headerlink" title="ELT with BigQuery and Cloud Storage"></a>ELT with BigQuery and Cloud Storage</h2><p>In Google Cloud Platform, it is very easy to do ELT with<br><a href="https://cloud.netapp.com/ma-gcp-plp-premium-google-cloud-storage" target="_blank" rel="noopener">Cloud Storage</a><br>and <a href="https://cloud.google.com/bigquery" target="_blank" rel="noopener">BigQuery</a>. Using GCS and BigQuery, Felipe Hoffa demonstrated a lazy-loading of half a trillion Wikipedia page views.<br>(His articles can be found <a href="https://cloud.google.com/blog/products/gcp/bigquery-lazy-data-loading-sql-data-languages-ddl-and-dml-partitions-and-half-a-trillion-wikipedia-pageviews" target="_blank" rel="noopener">here</a> and <a href="https://medium.com/google-cloud/bigquery-lazy-data-loading-ddl-dml-partitions-and-half-a-trillion-wikipedia-pageviews-cd3eacd657b6" target="_blank" rel="noopener">here</a>)</p><p>In his post, Hoffa loaded the super large set of files with a simple format (space-separated) parsing with <code>REGEXP_EXTRACT</code> function.<br>I will use<br><a href="https://earthquake.usgs.gov/fdsnws/event/1/" target="_blank" rel="noopener">the USGS Earthquake event data</a> as an example dataset. The earthquake event data is a newline-separated JSON file that looks like this:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;type&quot;:&quot;Feature&quot;,</span><br><span class="line">  &quot;properties&quot;:&#123;</span><br><span class="line">    &quot;mag&quot;:1.1000000000000001,</span><br><span class="line">    &quot;place&quot;:&quot;117km NW of Talkeetna, Alaska&quot;,</span><br><span class="line">    &quot;time&quot;:1388620046501,</span><br><span class="line">    &quot;updated&quot;:1558392330681,</span><br><span class="line">    &quot;tz&quot;:-540,</span><br><span class="line">    ...</span><br><span class="line">    &quot;type&quot;:&quot;earthquake&quot;,</span><br><span class="line">    &quot;title&quot;:&quot;M 1.1 - 117km NW of Talkeetna, Alaska&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;geometry&quot;:&#123;</span><br><span class="line">    &quot;type&quot;:&quot;Point&quot;,</span><br><span class="line">    &quot;coordinates&quot;:[</span><br><span class="line">      -151.64590000000001,</span><br><span class="line">      63.101999999999997,</span><br><span class="line">      14.1</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;id&quot;:&quot;ak01421ig3u&quot;</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">  &quot;type&quot;:&quot;Feature&quot;,</span><br><span class="line">  &quot;properties&quot;:&#123;</span><br><span class="line">    &quot;mag&quot;:1.2,</span><br><span class="line">    &quot;place&quot;:&quot;6km SSW of Big Lake, Alaska&quot;,</span><br><span class="line">    &quot;time&quot;:1388619956476,</span><br><span class="line">    &quot;updated&quot;:1558392330249,</span><br><span class="line">    &quot;tz&quot;:-540,</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line">    &quot;type&quot;:&quot;earthquake&quot;,</span><br><span class="line">    &quot;title&quot;:&quot;M 1.2 - 6km SSW of Big Lake, Alaska&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;geometry&quot;:&#123;</span><br><span class="line">    &quot;type&quot;:&quot;Point&quot;,</span><br><span class="line">    &quot;coordinates&quot;:[</span><br><span class="line">      -150.01650000000001,</span><br><span class="line">      61.458100000000002,</span><br><span class="line">      44.600000000000001</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;id&quot;:&quot;ak01421i2zj&quot;</span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure><h2 id="stdin-to-GCS"><a href="#stdin-to-GCS" class="headerlink" title="stdin to GCS"></a>stdin to GCS</h2><p>I wrote <a href="https://github.com/anelendata/target_gcs" target="_blank" rel="noopener">target_gcs</a>,<br>a Python program that takes stdin and writes out to a Cloud Storage location. It is easy to install via pip:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python3 -m venv .&#x2F;venv</span><br><span class="line">source .&#x2F;venv&#x2F;bin&#x2F;activate</span><br><span class="line">pip install https:&#x2F;&#x2F;github.com&#x2F;anelendata&#x2F;target_gcs&#x2F;tarball&#x2F;master</span><br></pre></td></tr></table></figure><p>Then follow the <a href="https://github.com/anelendata/target_gcs/blob/master/README.md#configure" target="_blank" rel="noopener">instruction</a> to configure the Google Cloud Platform project and data location.</p><p>After the simple set up, you can run</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl &quot;https:&#x2F;&#x2F;earthquake.usgs.gov&#x2F;fdsnws&#x2F;event&#x2F;1&#x2F;query?format&#x3D;geojson&amp;starttime&#x3D;2020-06-24&amp;endtime&#x3D;2020-06-25&quot; | \</span><br><span class="line">target_gcs -c .&#x2F;your-config.json</span><br></pre></td></tr></table></figure><p>to get the data loaded on Cloud Storage.<br>In this simple example, the USGS data is fetched by <code>curl</code> command,<br>and the data is written out to stdout.<br>target_gcs receives the data through pipe (<code>|</code>) and writes out to GCS.</p><h2 id="GCS-to-BigQuery"><a href="#GCS-to-BigQuery" class="headerlink" title="GCS to BigQuery"></a>GCS to BigQuery</h2><p>After the data is loaded to Cloud Storage, it’s time to create a table on BigQuery.<br>For this, we can create an <a href="https://cloud.google.com/bigquery/docs/hive-partitioned-loads-gcs" target="_blank" rel="noopener">externally partitioned table</a><br>with the <a href="https://github.com/anelendata/target_gcs/blob/master/create_schemaless_table.py" target="_blank" rel="noopener">Python script</a> I provided (*):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">python create_schemaless_table.py \</span><br><span class="line">  -p your-project-id \</span><br><span class="line">  -g gs:&#x2F;&#x2F;your-bucket&#x2F;your-dataset \</span><br><span class="line">  -d your-dataset-name \</span><br><span class="line">  -t your-table-name</span><br></pre></td></tr></table></figure><p>In my case, I set the target dataset name to <code>gcs_partitioned</code><br>and table name to <code>usgs_events_unparsed</code>.</p><p>(*) Note: If the JSON schema in the data is stable and consistent, it makes more sense to load in<br><a href="https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json" target="_blank" rel="noopener">the standard way of loading JSON data</a>.<br>In the method in this article is advantageous when you have JSON record whose set of keys are inconsistent among records.<br>I also had a case where I needed to first search for all the keys present in the data, then flatten/unwrap/unpivot the data into a long format.</p><h2 id="Unstructured-data-on-BigQuery"><a href="#Unstructured-data-on-BigQuery" class="headerlink" title="Unstructured data on BigQuery"></a>Unstructured data on BigQuery</h2><p>Let’s query the unstructured data.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SELECT *</span><br><span class="line">FROM &#96;gcs_partitioned.usgs_events_unparsed&#96;</span><br><span class="line">WHERE date BETWEEN DATE &#39;2020-06-24&#39; AND DATE &#39;2020-06-25&#39; LIMIT 10</span><br></pre></td></tr></table></figure><p><img src="/images/usgs_unparsed_sample_bigquery.png" alt=""></p><p>You can see <code>line</code> as the raw JSON string. You also see columns such as <code>version</code>, <code>format</code>, and <code>date</code>.<br>They are from the GCS file partition information. By using those keys, we can limit the total data scanned with a query.<br>In fact, the Python script we used to create the BigQuery table from GCS set the option to always require the partition filter in the query.<br>This is a good practice to avoid running a costly query by accident. (BigQuery costs $5 per TB data scanned.)</p><h2 id="Extract-the-fields-from-JSON-to-create-a-structured-view"><a href="#Extract-the-fields-from-JSON-to-create-a-structured-view" class="headerlink" title="Extract the fields from JSON to create a structured view"></a>Extract the fields from JSON to create a structured view</h2><p>We can create a view that has the structure from the unstructured table:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">CREATE VIEW &#96;views.usgs_events&#96; AS (</span><br><span class="line">  SELECT</span><br><span class="line">    STRUCT(</span><br><span class="line">      TIMESTAMP_MILLIS(</span><br><span class="line">         CAST(JSON_EXTRACT(line,</span><br><span class="line">          &#39;$.properties.time&#39;) AS INT64)) AS time,</span><br><span class="line">      CAST(JSON_EXTRACT(line,</span><br><span class="line">          &#39;$.properties.tz&#39;) AS INT64) AS tz,</span><br><span class="line">      CAST(JSON_EXTRACT(line,</span><br><span class="line">          &#39;$.properties.mag&#39;) AS FLOAT64) AS mag,</span><br><span class="line">      TRIM(JSON_EXTRACT(line,</span><br><span class="line">          &#39;$.properties.place&#39;)) AS place ) AS properties,</span><br><span class="line">      CONCAT(JSON_EXTRACT_ARRAY(line, &#39;$.geometry.coordinates&#39;)[OFFSET(1)], &#39;,&#39;,</span><br><span class="line">             JSON_EXTRACT_ARRAY(line, &#39;$.geometry.coordinates&#39;)[OFFSET(0)]) AS lat_lng,</span><br><span class="line">    date AS etl_date</span><br><span class="line">  FROM &#96;gcs_partitioned.usgs_events_unparsed&#96;</span><br><span class="line">  WHERE JSON_EXTRACT(line, &#39;$.type&#39;) &#x3D; &#39;&quot;Feature&quot;&#39;</span><br><span class="line">  )</span><br></pre></td></tr></table></figure><p>Here, I’m using functions such as <code>JSON_EXTRACT</code> and <code>JSON_EXTRACT_ARRAY</code> to extract the values from the nested field to create the flat table.<br>I’m using <code>CAST</code> function to convert the extracted string into the appropriate data types:</p><p><img src="/images/usgs_events_view.png" alt=""></p><p>Note that latitude and longitude are concatenated as a comma-separated string for the next step.</p><p>Also note that I renamed <code>date</code> partition field to <code>etl_date</code>.<br>The view inherits the partition filter requirement so we can avoid any accident of scanning the entire dataset by accident:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SELECT * FROM &#96;views.usgs_events&#96;</span><br><span class="line">WHERE etl_date BETWEEN DATE &#39;2020-06-24&#39; and DATE &#39;2020-06-25&#39;</span><br></pre></td></tr></table></figure><p><img src="/images/usgs_parsed_events_sample.png" alt=""></p><p>Once the data is formatted, it is straightforward to visualize it on Google Data Studio:</p><p><img src="/images/usgs_events_map.png" alt=""></p><p>Here, Google Maps are selected as the chart type.<br><code>lag_lng</code> is used for bubble location,<br>and <code>property.mag</code> is used as the bubble size.</p><h2 id="ELT-Simple-and-powerful"><a href="#ELT-Simple-and-powerful" class="headerlink" title="ELT: Simple and powerful"></a>ELT: Simple and powerful</h2><p>In this post, I extracted the data simply with curl command without worrying about the file format or schema. The data was just dumped to Cloud Storage.<br>Then I created an externally partitioned table. Here again, I did not care to specify a schema.<br>At the very last step, I extracted the values from the raw strings and saved them as a structured view. I did so just by writing a SQL.<br>I could extract what I want with <code>JSON_EXTRACT</code> function for newline-separated JSON records. <code>REGEXP_EXTRACT</code> function would be useful for other types of file formats.</p><p>The parallelization of the query execution comes free, thanks to BigQuery.<br>Just imagine how slow the process would be if I had to</p><ol><li>Specify the schema.</li><li>Write the transformation logic.</li><li>Configure the parallel processing for the transformation process by myself.</li></ol><p>…before loading the data to the data warehouse.<br>What if the raw data’s schema suddenly changed?</p><p>I hope you are impressed by how quickly I processed and visualized the earthquake event data by choosing the simple and powerful ETL approach.<br>The computational benefit of transforming data in a massively-parallel data warehouse such as BigQuery would be even greater once I start extracting much more data continuously with a production-level extraction process such as <a href="https://articles.anelen.co/kinoko-io-etl/" target="_blank" rel="noopener">this one</a>.</p><h2 id="A-word-of-caution"><a href="#A-word-of-caution" class="headerlink" title="A word of caution"></a>A word of caution</h2><p>I just demonstrated a case of efficiency gain with ELT.<br>However, ETL or ETLT can be more appropriate in other scenarios.<br>For example, masking or obfuscating personal identifiable data is very important before storing them in a data lake including Cloud Storage because it is much more costly to delete or modify a single record in a file partition after it’s loaded. (Think of a GDPR case a user requests a complete data erase.)</p><h2 id="Extra-Animate-the-earthquake-events-on-the-map"><a href="#Extra-Animate-the-earthquake-events-on-the-map" class="headerlink" title="Extra: Animate the earthquake events on the map"></a>Extra: Animate the earthquake events on the map</h2><p><img src="/images/earthquakes_20200624.gif" alt="earthquakes"></p><p>Above visualization was created with this <a href="https://colab.research.google.com/drive/1Ajf3VzqPxJpu9Vrq5EZTDRGRxP6HjXC0?usp=sharing" target="_blank" rel="noopener">source code</a></p><p>I hope this introduction was informative and relatable to your business. If you would like to find out more what Anelen can do for your data challenge, please contact <a href="mailto:hello@anelen.co">hello@anelen.co</a> (.co, not .com don’t get your email lost elsewhere!) or schedule a free discovery meeting with the <a href="https://calendly.com/anelen-discovery" target="_blank" rel="noopener">scheduler</a>.</p><p>Daigo Tanaka, CEO and Data Scientist @ ANELEN</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/images/earthquakes.png&quot; alt=&quot;USGS earthquake events map&quot;&gt;&lt;/p&gt;
&lt;p&gt;The image above, generated with the data in this article,&lt;br&gt;
      
    
    </summary>
    
    <content src="http://handoff.cloud//images/earthquakes.png" type="image" />
    
    
    
  </entry>
  
</feed>
